<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>EM algorithm</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/black.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
		<script defer src="https://pyscript.net/latest/pyscript.js"></script>
		<py-config>
			terminal = false
#            packages = ["numpy", "pandas", "matplotlib", "scikit-learn"]
		</py-config>
		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
					<div>
						<img src="./images/nup_logo_dark.jpeg" alt="nup_logo" />
					</div>
                    <h2>Machine learning hard</h2>
					<div class="fragment" style="margin-bottom:20px;">
							<div class="typesetting">
								<h3>Expectetion-Minimization algorithm</h3>
								<br />
								Alex Avdiushenko<br />
								February 14, 2024
							</div>
					</div>
                </section>
                <section>
                    <section data-background-color="#ffffff">
                        <img src="images/kl_div_mem.jpg" alt="kl_div_mem" />
                    </section>
                    <section>
                        <h2 style="text-align: left">KL divergence</h2>
                        <div class="fragment">
                            <div class="typesetting">
                                <h3>Jensen's Inequality</h3>
                                <div class="row">
                                    <div class="column">
                                        <p>If \( f(x) \) is a concave (upward-convex) function, then Jensen's inequality holds:</p>
                                        <p>\[ f(wx_1 + (1 − w)x_2) \geq \color{orange}{wf(x_1) + (1 − w)f(x_2)} \]</p>
                                    </div>
                                    <div class="column">
                                        <img style="padding-left: 20%" src="images/concave_function.jpg" alt="concave_function" />
                                    </div>
                                </div>

                                <p style="text-align: left">Or, in a more general case,</p>
                                <p>\[f(w_1x_1 + w_2x_2 + \dots + w_nx_n) \geq w_1f(x_1) + w_2f(x_2) + \dots + w_nf(x_n)\]</p>
                                <p style="text-align: left">where \( \sum\limits_{i=1}^n w_i = 1 \). It can also be proven that
                                    \(  f\left(\mathbb{E}_{p(x)} u(x)\right) \geq \mathbb{E}_{p(x)} f(u(x)) \)</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Kullback-Leibler Divergence Between Probability Distributions</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p>\[ KL(p||q) = \int p(x) \log \frac{p(x)}{q(x)} dx \]</p>
                                <p><b>Properties of Kullback-Leibler Divergence:</b></p>
                                <ul>
                                    <li>\( KL(p||q) \neq KL(q||p) \) — asymmetry</li>
                                    <li>\( KL(p||q) \geq 0 \) — non-negativity</li>
                                    <li>\( KL(p||q) = 0 \Leftrightarrow q(x) = p(x) \)</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Proof of Non-negativity of KL Divergence</h3>
                        \[ -KL(p||q) = -\int p(x) \log\left(\frac{p(x)}{q(x)}\right)dx = \int p(x) \log\left(\frac{q(x)}{p(x)}\right)dx \leq \\
                        \leq \log\left( \int p(x) \frac{q(x)}{p(x)}dx\right) = \log\left( \int q(x) dx\right) = \log 1 = 0 \]
                    </section>
                    <section>
                        <h3 style="text-align: left">Asymmetry of KL Divergence</h3>
                        <p style="text-align: left">Example: Searching for a Similar Distribution</p>

                        <p>\( KL(? || ?) \) — where to place \( p \)?</p>
                        <div>
                            <img style="border-radius: 5%" src="images/example.jpg" alt="example_1" width="600" />
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Asymmetry of KL Divergence. First Argument: Mass Covering</h3>
                        <p>\( KL(p || q) = \int p(x) \log \frac{p(x)}{q(x)} dx \)</p>
                        <div>
                            <img style="border-radius: 5%" src="images/example_1.jpg" alt="example_1" width="600" />
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Asymmetry of KL Divergence. Second Argument: Mode Collapsing</h3>
                        \[ KL(q || p) = \int q(x) \log \frac{q(x)}{p(x)} dx \]
                        <div>
                            <img style="border-radius: 5%" src="images/example_2.jpg" alt="example_2" width="600" />
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Reminders and the Exponential Family of Distributions</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>It is assumed that in linear regression, the responses are generated from a normal distribution:<br>
                                    \[ y_i \sim N(\mu_i, \sigma_i^2),\ \ \ \mu_i = E y_i = x_i^T w \]</li>
                                    <li>Generalized linear model for regression:<br>
                                    \( y_i \sim {\color{orange}Exp}(\mu_i, \phi_i),\ \ \ \mu_i = E y_i,\ \ \ {\color{orange}g(\mu_i)} = \theta_i = g(x_i^T w) \)<br>
                                    \( g(\mu_i) \) — a monotonic link function<br>
                                    \( Exp \) — the exponential family of distributions with parameters \( \theta_i, \phi_i \) and function-parameters \( c(\theta), h(y, \phi) \), thus with density<br>
                                    \( p(y_i | \theta_i, \phi_i) = \exp\left(\frac{y_i\theta_i - c(\theta_i)}{\phi_i} + h(y_i, \phi_i)\right) \)<br>
                                    \( \theta_i = g\left(\sum\limits_{j=1}^n w_j f_j(x_i) \right) \)</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section data-background-color="#ffffff">
                        <div>
                            <img src="images/you-wont-be-disappointed-if-you-have-no-expectation.jpg" alt="exp_mem" width="800" />
                        </div>
                    </section>
                    <section>
                        <h3>EM-Algorithm: Motivating Example</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">How can we learn to reconstruct distributions?</p>

                                <p>Gaussian:</p>
                                <div>
                                    <img src="images/gauss_1.jpg" alt="Gaussian distribution" width="400" />
                                </div>

                                <p>Multiple Gaussians:</p>
                                <div>
                                    <img src="images/gauss_3.jpg" alt="Multiple Gaussian distributions" width="400" />
                                </div>

                                <p>Unknown Distribution:</p>
                                <div>
                                    <img src="images/gauss_unknown.jpg" alt="Unknown distribution" width="400" />
                                </div>

                                <p>We can introduce the Gaussian number and reduce the problem to
                                    an already solved one (as in joke with electric kettle, yeah)!</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Introducing Latent Variables</h3>

                        <p style="text-align: left">We want: $$p(X|\theta) \underset{\theta}{\to} \max \text{(incomplete likelihood)}$$</p>

                        <p style="text-align: left">We can: $$p(X, Z|\theta) \underset{\theta}{\to} \max \text{(complete likelihood)}$$</p>

                        <p>Let's say in the previous example, $z_i$ is the identifier of the Gaussian from which the $i$-th object is taken.</p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Variational Lower Bound</h3>

                        <p style="text-align: left">For a single observation $x$:</p>
                        $$
                        \begin{align*}
                        \log p(x|\theta) &= \int q(z) \log p(x|\theta) dz = \int q(z) \log \frac{p(x, z|\theta)}{p(z|x,\theta)} dz \\
                        &= \int q(z) \log \frac{p(x, z|\theta)q(z)}{p(z|x,\theta)q(z)} dz\\
                        &= \int q(z) \log \frac{p(x, z|\theta)}{q(z)} dz + \int q(z) \log \frac{q(z)}{p(z|x,\theta)} dz \geq\\
                        &\geq \int q(z) \log \frac{p(x, z|\theta)}{q(z)} dz = \mathscr{L}(q, \theta)
                        \end{align*}
                        $$
                    </section>
                    <section>
                        <div class="fragment">
                            <div class="typesetting">
                                This is the variational lower bound: we simply discarded the non-negative
                                $KL(q(z) || p(z|x,\theta))$.
                                Furthermore, equality is reached at $q^* = p(z| x, \theta)$

                                <p style="text-align: left">For sample $X$:</p>

                                $$ \log p(X| \theta) =
                                   \sum\limits_{i=1}^n \log p(x_i|\theta) \geq \sum\limits_{i=1}^n \mathscr{L}(q_i, \theta) \underset{q, \theta}{\to} \max
                                $$
                                <p>It's easy to maximize for $\theta$ if we know $q$ and vice versa.</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>EM-Algorithm</h3>

                        <p style="text-align: left">E-step (Expectation):</p>
                        $$
                        \sum\limits_{i=1}^n \mathscr{L}(q_i, \theta) \underset{q_i}{\to} \max \Leftrightarrow
                         q_i(z_i) = p(z_i|x_i, \theta)
                        $$

                        <p style="text-align: left">M-step (Maximization):</p>
                        $$
                        \begin{align*}
                        \max\limits_\theta \sum\limits_{i=1}^n \mathscr{L}(q_i, \theta) &= \max\limits_\theta \sum\limits_{i=1}^n \int q_i(z_i) \log\frac{p(x_i, z_i| \theta)}{q_i(z_i)} dz_i \\
                        &= \max\limits_\theta \sum\limits_{i=1}^n E_{q_i(z_i)} \log p(x_i, z_i| \theta)
                        \end{align*}
                        $$
                    </section>
                    <section>
                        <img style="border-radius: 5%" src="images/em.gif" alt="em" width=800 />
                    </section>
                    <section>
                        <h3>Gaussian Mixture Model</h3>

                        <div>
                            <img style="border-radius: 5%" src="images/GMM.jpg" alt="GMM" width="600" />
                        </div>
                    </section>
                    <section>
                        <div class="fragment">
                            <div class="typesetting">
                                <p>The joint probability of $x_i$ and $z_i$ given $\theta$ can be represented as:</p>

                                $$ p(x_i, z_i|\theta) = p(x_i|z_i,\theta)p(z_i|\theta)
                                    = N(x_i|\mu_{z_i}, \sigma_{z_i}^2) \pi_{z_i} $$

                                <p>Parameters: $\{\mu_j, \sigma_j, \pi_j\}$, here $\pi_j$ is a priori probability of the
                                    $j$-th Gaussian (hidden variable $z_i$)</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="r-frame">
                            <b>Question 1:</b> How do you find the number of mixture components?
                        </div>
                        <br>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>We can use simple enumeration based on their quantity (model selection method)</li>
                                    <li>We can take a different approach — introduce a prior distribution
                                        <b>directly on all models of different complexity</b>.
                                        This is the main idea of Bayesian nonparametric methods</li>
                                    <li>See the "Chinese Restaurant Process" and "Indian Buffet Process"</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>EM-Algorithm for Gaussian Mixture Model</h3>

                        <p style="text-align: left">E-step: $\mu_k, \sigma_k^2, \pi_k \to q$</p>

                        $$
                        q_i(z_i{=}j) = p(z_i{=}j|x_i, \theta) = \frac{p(x_i|z_i {=} j, \theta)p(z_i {=} j|\theta)}{p(x_i|\theta)} = \\
                           = \frac{p(x_i|z_i {=} j, \theta)p(z_i {=} j|\theta)}{\sum\limits_{k=1}^K p(x_i|z_i {=} k, \theta)p(z_i {=} k|\theta)}
                           = \frac{N(x_i|\mu_j, \sigma_j^2) \pi_j}{\sum\limits_{k=1}^K N(x_i|\mu_k, \sigma_k^2) \pi_k}
                        $$
                    </section>
                    <section>
                        <p style="text-align: left">M-step: $q \to \mu_k, \sigma_k^2, \pi_k$</p>

                        $$
                        \begin{align*}
                        \sum\limits_{i=1}^n \mathscr{L}(q,\theta) &= \sum\limits_{i=1}^n \sum\limits_{k=1}^K q_i(z_i=k) \log\frac{p(x_i|z_i = k, \theta)p(z_i = k|\theta)}{q_i(z_i=k)} \\
                        &= \sum\limits_{i=1}^n \sum\limits_{k=1}^K q_i(z_i=k) \log\frac{N(x_i|\mu_k, \sigma_k^2)\pi_k}{q_i(z_i=k)}
                        \end{align*}
                        $$
                    </section>
                    <section>
                        <p style="text-align: left">Next, as usual, we find the zeros of the derivative:</p>

                        $$
                        \mu_j = \frac{\sum\limits_{i=1}^n q_i(z_i=j)x_i}{\sum\limits_{i=1}^n q_i(z_i=j)};\
                        \sigma_j^2 = \frac{\sum\limits_{i=1}^n q_i(z_i=j)(x_i-\mu_j)^2}{\sum\limits_{i=1}^n q_i(z_i=j)}\\
                        \pi_j = \frac{1}{n} \sum\limits_{i=1}^n q_i(z_i = j)
                        $$
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Principal Component Analysis: Problem Formulation</h3>

                        <p style="text-align: left">$f_1(x), \dots, f_D(x)$ — original numerical features</p>

                        <p style="text-align: left">$g_1(x), \dots, g_d(x)$ — new numerical features, $d \leq D$</p>

                        <p><b>Requirement</b>: The old features should be restored as accurately as possible
                            linearly from the new ones on the training set $X^n$:</p>
                        $$ \hat f_j(x) = \sum\limits_{s=1}^d g_s(x) u_{js}, j=1,\dots,D , \forall x \in X $$
                        $$ \sum\limits_{i=1}^n \sum\limits_{j=1}^D (\hat f_j(x_i) - f_j(x_i))^2 \to \min\limits_{\{g_s(x_i)\}, \{u_{js}\}} $$
                    </section>
                    <section>
                        <h3>Matrix Notations</h3>

                        <p style="text-align: left">Old and new "object-feature" matrices:</p>

                        $$
                        F_{n \times D} =
                        \left[ \begin{array}{ccc}
                           f_1(x_1) & \dots & f_D(x_1) \\
                             \dots  & \dots &   \dots  \\
                           f_1(x_n) & \dots & f_D(x_n)
                          \end{array} \right]
                        \ ,\\[.5cm]
                        G_{n \times d} =
                        \left[ \begin{array}{ccc}
                           g_1(x_1) & \dots & g_d(x_1) \\
                             \dots  & \dots &   \dots  \\
                           g_1(x_n) & \dots & g_d(x_n)
                          \end{array} \right]
                        $$
                    </section>
                    <section>
                        <p>The transformation of old features into new ones should be linear:</p>

                        $$
                        U_{D \times d} =
                        \left[ \begin{array}{ccc}
                           u_{11} & \dots & u_{1d} \\
                             \dots  & \dots &   \dots  \\
                           u_{D1} & \dots & u_{Dd}
                          \end{array} \right],
                          \hat F = GU^T \underset{\text{desired}}{\approx} F
                        $$

                        <p style="text-align: left">
                            <b>To find</b>: both new features $G$, and transformation $U$:</p>
                        $$
                          \sum\limits_{i=1}^n \sum\limits_{j=1}^D (\hat f_j(x_i) - f_j(x_i))^2 = \| GU^T - F\|^2 \to \min\limits_{G,U}
                        $$
                    </section>
                    <section>
                        <h3>Probabilistic Interpretation of the Principal Component Analysis (PPCA)</h3>

                        $$
                        p(x,z|\theta) = p(x|z, \theta)p(z) = N(x|Wz + \mu, \sigma^2I)N(z|0, I),
                        $$

                        <p style="text-align: left">where $x \in R^D$, $z \in R^d$ and $z \sim N(0, I)$</p>

                        <p style="text-align: left">The parameters are ${W, \mu, \sigma}$,
                            where $\mu \in R^D$, $W \in R^{D\times d}$</p>

                        <div class="r-frame">
                            <b>Note:</b> Constraints can also be imposed on the matrix $W$,
                            expressed by the prior distribution, to minimize $d$ during optimization.
                        </div>
                    </section>
                    <section>
                        <h3>EM-Algorithm for Probabilistic Principal Component Analysis</h3>

                        <p style="text-align: left">E-step (is calculated analytically, as conjugate distributions exist):</p>

                        $$
                        q_i(z_i) = p(z_i|x_i, \theta) = \frac{p(x_i|z_i, \theta)p(z_i)}{\int p(x_i|z_i, \theta)p(z_i) dz_i} = \\[.2cm]
                          = N(z_i| (\sigma^2I + W^TW)^{-1}W^T (x_i - \mu), (I + \sigma^{-2}W^TW)^{-1})
                        $$

                        <p style="text-align: left">M-step:</p>

                        $$
                        E_Z\log p(X, Z|\theta) = \sum\limits_{i=1}^n E_{z_i} \log p(x_i, z_i|\theta) = \\
                          \sum\limits_{i=1}^n E_{z_i} \log p(x_i| z_i, \theta) + const \underset{\theta}{\to} \max
                        $$
                    </section>
                    <section>
                        <h3 style="text-align: left">M-step example: finding W</h3>

                        $$
                        \sum\limits_{i=1}^n E_{z_i} \log p(x_i| z_i, \theta) = \\
                         = \sum\limits_{i=1}^n E_{z_i} \left[-\frac{D}{2}\log2\pi-D\log\sigma-\frac{1}{2\sigma^2}\left((x_i - \mu)-Wz_i\right)^T\left((x_i - \mu)-Wz_i\right) \right] = \\
                         = const(\sigma) - \frac{1}{2\sigma^2} \sum\limits_{i=1}^n E_{z_i} \left[(x_i - \mu)^T(x_i - \mu) - 2(x_i - \mu)^T Wz_i + z_i^TW^T Wz_i \right] = \\
                         = const(\sigma) - \frac{1}{2\sigma^2} \sum\limits_{i=1}^n \left[(x_i - \mu)^T(x_i - \mu) - 2(x_i - \mu)^T W E_{z_i}z_i + tr(W^T WE_{z_i}z_iz_i^T) \right]
                        $$
                    </section>
                    <section>
                        <p style="text-align: left">Next, differentiate with respect to $W$:</p>
                        $$
                            - \frac{1}{2\sigma^2} \sum\limits_{i=1}^n \left[-2(x_i - \mu) E_{z_i} z_i^T + 2W E_{z_i} z_i z_i^T \right] = 0 \\
                            W^* = \left( \sum\limits_{i=1}^n (x_i - \mu) E_{z_i} z_i^T \right) \left( \sum\limits_{i=1}^n E_{z_i} z_i z_i^T \right)^{-1}
                        $$

                        <p>We can calculate the expectations since we know the distribution $q$ from the previous E-step.</p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Comparison of Runtime Estimates</h3>

                        <p style="text-align: left">When $n > D > d$:</p>
                        <ul>
                          <li>Standard PCA: $O(nD^2)$</li>
                          <li>A single iteration of PPCA: $O(nDd)$</li>
                        </ul>

                        <p>Usually, 10-100 iterations are needed for convergence, so it may be faster than the standard method</p>
                    </section>
                    <section>
                        <h3>The case of missing features in Probabilistic Principal Component Analysis</h3>

                        $$
                        (X, Z) = X_{\text{obs}} \cup X_{\text{hid}},
                           P(X_{\text{obs}}|\theta) \underset{\theta}{\to} \max
                        $$

                        <p style="text-align: left">E-step</p>

                        $$
                        q(X_{\text{hid}}) = p(X_{\text{hid}}|X_{\text{obs}}, \theta)
                        $$

                        <p style="text-align: left">M-step</p>

                        $$
                        E_{X_\text{hid}} \log p(X_{\text{obs}}, X_{\text{hid}} |\theta) \underset{\theta}{\to} \max
                        $$

                        <p style="text-align: left">And we also obtain suitable probabilistic distributions for hidden variables</p>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Summary</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                  <li>A common metric between probabilistic distributions is the Kullback-Leibler distance (KL-divergence).</li>
                                  <li>The EM algorithm allows iterative solutions to complex problems by dividing them into two steps.</li>
                                  <li>Probabilistic Principal Component Analysis using the EM algorithm.</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>