<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>Generative and discriminative models</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/black.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
					<div>
						<img src="./images/nup_logo_dark.jpeg" alt="nup_logo" />
					</div>
                    <h2>Advanced machine learning</h2>
					<div class="fragment" style="margin-bottom:20px;">
							<div class="typesetting">
								<h3>Generative and discriminative models</h3>
								<br />
								Alex Avdiushenko<br />
								March 26, 2024
							</div>
					</div>
                </section>
                <section>
                    <section>
                        <h3>Supervised Learning Task Formulation</h3>

                        <p style="text-align: left"><strong>Given: </strong>set of objects $X$, set of answers $Y$</p>

                        <p style="text-align: left"><strong>We need to find</strong> $p(y | x)$ distribution — it is discriminative model</p>

                        <br>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">In Generative model, we need to find joint distribution $p(x, y)$ and
                                $$p(y | x) \propto  p(x, y)$$</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>What is the difference?</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">On the one hand, Discriminative models cannot generate objects.
                                    On the other hand, they are much more expressive because
                                    they do not make any internal assumptions about the structure of the object's feature space $X$.
                                </p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Generative models</h3>
                        <h4 class="fragment" style="text-align: left">Explicit density models</h4>
                        <h5 class="fragment" style="text-align: left">1. Tractable density models</h5>
                            <div class="fragment">
                                <div class="typesetting">
                                        <p style="text-align: left">Naive Bayes</p>
                                        $$ p(x, y) = p(y) \prod\limits_{i=1}^n p(x_i | y) $$

                                        <p style="text-align: left">Autoregressive models (including Transformers)</p>
                                        $$ p(x_1, \dots, x_n) = p(x_1) p(x_2|x_1) \dots p(x_n | x_1, \dots, x_{n-1}) $$
                                </div>
                            </div>
                    </section>
                    <section>
                        <h5 class="fragment" style="text-align: left">2. Approximate density models</h5>
                            <div class="fragment">
                                <div class="typesetting">
                                    $$ p(x) \approx q(x) = \dots, \quad KL(q \| p) \to \min $$
                                    <p style="text-align: left">Diffusion based</p>
                                    $$ x \to x + \varepsilon_1 \to \dots \to x_n \sim N(0, \Sigma) $$

                                    <p style="text-align: left">Variational auto encoders — VAE</p>
                                </div>
                            </div>
                    </section>
                    <section>
                        <h4 style="text-align: left">Implicit density models</h4>
                            <div class="fragment">
                                <div class="typesetting">
                                        <p style="text-align: left">Generative Adversarial Networks — GANs</p>
                                        $$ z \sim N(0, \Sigma) \to \boxed{\text{Model}} \to x$$
                                </div>
                            </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Autoencoders</h3>
                        <img src="images/autoencoder-architecture.png" style="width:80%;border-radius: 5%" alt="Autoencoder Architecture">
                        <p><a href="https://lilianweng.github.io/posts/2018-08-12-vae/">Source: Lilian Weng post</a></p>

                        <aside class="notes">
                            Now let's continue with another interesting approach in machine learning.
                            It is autoencoders and its main idea is quite simple.
                            So you have some input vector, for example, an image.

                            Then you have two parts of the model, which are the encoder part,
                            where you get some representation of the input, and the decoder part,
                            where you try to get back to the initial input vector.
                        </aside>
                    </section>
                    <section>
                        <h3>Ways to use autoencoders</h3>
                            <div class="fragment">
                                <div class="typesetting">
                                    <ul style="text-align:left;">
                                        <li>Feature generation, for example, to effectively solve supervised learning problems</li>
                                        <li>Dimensionality reduction</li>
                                        <li>Low loss data compression</li>
                                        <li>Trainable object vectorization, embeddable in deeper neural network architectures</li>
                                        <li>Generation of synthetic objects similar to real ones</li>
                                    </ul>
                                </div>
                            </div>

                            <div class="fragment">
                                <div class="typesetting">
                                    <hr style="width: 100%; margin: 20px auto;"/>

                                    <p style="text-align: left;font-size: smaller">Rumelhart, Hinton, Williams.
                                        Learning Internal Representations by Error Propagation. 1986.</p>
                                    <p style="text-align: left;font-size: smaller">David Charte et al.
                                        A practical tutorial on autoencoders for nonlinear feature fusion:
                                        taxonomy, models, software and guidelines. 2018.</p>
                                </div>
                            </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Linear Auto Encoder and Principal Component Analysis</h3>
                        $$ \mathscr{L}_{AE}(A, B) = \sum\limits_{i = 1}^{\ell} \|{\color{orange}BA}x_i - x_i \|^2 \to \min \limits_{A,B}$$
                        <p style="text-align: left">Principal Component Analysis:
                            $F = (x_1 \dots x_{\ell})^T, U^TU = I_m, G = FU,$</p>
                        $$\|F - GU^T \|^2 = \sum\limits_{i = 1}^{\ell} \|{\color{orange}UU^T}x_i - x_i \|^2 \to \min\limits_{U}$$
                    </section>
                    <section>
                        <p style="text-align: left">The autoencoder generalizes the principal component analysis:</p>
                        <div class="fragment">
                            <div class="typesetting">
                            <ul style="text-align:left">
                                <li>it is not necessarily $B=A^T$ (although this is often done)</li>
                                <li>arbitrary $A, B$ instead of orthogonal</li>
                                <li>non-linear models</li>
                                <li>arbitrary loss function $\mathscr{L}$ instead of quadratic</li>
                                <li>SGD optimization instead of singular value decomposition (SVD)</li>
                            </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Sparse Auto Encoder</h3>
                        <p class="r-frame" style="text-align: left">
                            Reminder from linear models: if the loss function has a kink,
                            then we select objects. If regularizer has a kink, then we select features.
                        </p>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul style="text-align:left;">
                                    <li>Applying $L_1$ or $L_2$ regularization to weight vectors</li>
                                    <li>Applying of $L_1$-regularization to representation vectors $z_i = Ax_i$</li>
                                    <li>Entropy regularization</li>
                                </ul>

                                <br><br>
                                <hr style="width: 100%; margin: 20px auto;"/>
                                <p style="text-align: left;font-size: smaller">
                                    D.Arpit et al. Why regularized auto-encoders learn sparse representation? 2015.</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <img src="images/2d-font-manifold.png" style="width:70%;border-radius: 5%" alt="2D Font Manifold">
                        <p>
                            <a href="https://www.ndfcampbell.org/research/fonts/">2D font manifold demonstration</a>
                        </p>
                        <aside class="notes">
                            Here let's enjoy a little demo where different fonts are nested in a 2D plane.
                        </aside>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Variational Auto Encoder</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">A generative model is constructed capable of generating new objects
                                    $x$ similar to the objects of the sample $X^\ell = \{x_1,\dots,x_\ell \}$</p>
                                <p>$q_\alpha(z|x)$ — probabilistic encoder with $\alpha$ parameter</p>
                                <p>$p_\beta(\hat x|z)$ — probabilistic decoder with $\beta$ parameter</p>
                                $$
                                \begin{align*}
                                \mathscr{L}_{VAE}(\alpha, \beta) = \sum\limits_{i=1}^\ell \log p(x_i) = \sum\limits_{i=1}^\ell \log \int q_{\alpha} (z | x_i) \frac{p_{\beta}(x_i|z) p(z)}{q_{\alpha} (z | x_i)} dz \geq \\
                                 \geq \sum\limits_{i=1}^\ell \int q_\alpha(z|x_i) \log p_\beta(x_i|z)dz - KL(q_\alpha(z|x_i)\| p( z)) \to \max\limits_{\alpha, \beta}
                                \end{align*}
                                $$
                            </div>
                        </div>

                        <aside class="notes">
                            The next one is Variational Auto Encoder, and it may be challenging to understand from the first view,
                            but its main feature is to use lower variational bound of loglikelihood for your model.
                        </aside>
                    </section>
                    <section>
                        $$
                        \sum\limits_{i=1}^\ell
                        \underbrace{E_{z \sim q_{\alpha}(z|x_i)} \log p_\beta(x_i|z)}_{\text{quality reconstruction}} -
                        \underbrace{KL(q_\alpha(z|x_i)\| p(z))}_{\text{regularizer by } \alpha}
                        \to \max\limits_{\alpha, \beta}
                        $$
                        <p style="text-align: left">
                            where $p(z)$ is the prior distribution, usually $N(0, \sigma^2 I)$</p>

                        <br><br>
                        <p>Reparametrization $q_\alpha (z|x_i):\ z = f(x_i, \alpha, \varepsilon),\ \varepsilon \sim N(0, I)$</p>

                        <aside class="notes">
                            So we have two parts of functional,
                            first one is responsible for quality reconstruction, second one is actually regularizer by $\alpha$.

                            To evaluate the expectation of some function with z distribution, you need to use
                            reparametrization trick. What is it?

                            In the first step, you sample z-value as value of new function f with new parameter $\epsilon$,
                            which comes from some random distribution, for example, normal.
                            And then on the second step you get the expectation with your z-value.
                        </aside>
                    </section>
                    <section data-background-color="antiquewhite">
                        <img src="images/vae_trick.png" style="border-radius: 5%" width="90%">

                        <p style="text-align: center;font-size: smaller">
                            <a href="https://snawarhussain.com/blog/genrative%20models/python/vae/tutorial/machine%20learning/Reparameterization-trick-in-VAEs-explained/">
                                Source: Snawar Hussain blog</a></p>

                    </section>
                    <section>
                        <p style="text-align: left">Stochastic gradient method:</p>
                        <ul>
                            <li>sample $x_i \sim X^\ell,\ \varepsilon \sim N(0, I),\ z = f(x_i, \alpha, \varepsilon)$</li>
                            <li>gradient step $ \alpha = \alpha + h \nabla_\alpha[\log p_\beta(x_i|f(x_i, \alpha, \varepsilon)) - KL(q_\alpha(z|x_i)\| p(z)) ] $</li>
                            <li>gradient step $ \beta = \beta + h \nabla_\beta[\log p_\beta(x_i|z)] $</li>
                        </ul>

                        <p style="text-align: left">
                            Generation of similar objects:</p>
                            $$x \sim p_\beta(x|f({\color{orange}x_i}, \alpha, \varepsilon)), \varepsilon \sim N(0, I)$$

                        <hr style="width: 100%; margin: 20px auto;"/>
                        <p style="text-align: left;font-size: smaller">
                            D.P.Kingma, M.Welling. Auto-encoding Variational Bayes. 2013.</p>
                        <p style="text-align: left;font-size: smaller">
                            C.Doersch. Tutorial on variational autoencoders. 2016.</p>
                    </section>
                    <section>
                        <h3>Autoencoders for Supervised Learning</h3>
                        <p style="text-align: left">Data: unlabeled $(x_i)_{i=1}^\ell$, labeled $(x_i, y_i)_{i=\ell+1}^{\ell + k}$</p>
                        <div class="column">
                            $$
                            \begin{align*}
                              z_i &= f(x_i, {\color{blue}\alpha}) \text{ — encoder} \\
                              \hat x_i &= g(z_i, {\color{green}\beta}) \text{ — decoder} \\
                              \hat y_i &= \hat y(z_i, {\color{orange}\gamma}) \text{ — classifier}
                            \end{align*}
                            $$
                        </div>
                        <div class="column">
                            <img src="images/autoenc_supervised.png" alt="Autoencoders for Supervised Learning"
                                 width="80%" style="border-radius: 5%">
                        </div>
                    </section>
                    <section>
                        <p>Co-learning encoder, decoder and predictive model (classification, regression, etc.)</p>
                        $$
                            \sum\limits_{i=1}^\ell \mathscr{L}(g(f(x_i, \alpha), \beta), x_i)
                            + \lambda \sum\limits_{i=\ell+1}^{\ell+k} \tilde{\mathscr{L}}(\hat y(f(x_i, \alpha), \gamma), y_i)
                            \to \min\limits_{\alpha, \beta, \gamma}
                        $$

                        <p style="text-align: left">Loss functions:</p>
                        <ul>
                            <li>$\mathscr{L}(\hat x_i, x_i)$ — reconstruction</li>
                            <li>$\tilde{\mathscr{L}}(\hat y_i, y_i)$ — prediction</li>
                        </ul>

                        <hr style="width: 100%; margin: 20px auto;"/>
                        <p style="text-align: left;font-size: smaller">
                            Dor Bank, Noam Koenigstein, Raja Giryes. Autoencoders. 2020.</p>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Generative Adversarial Net (GAN)</h3>
                        <p style="text-align: left">The generator $G(z)$ learns to generate objects $x$ from noise $z$.
                        The discriminator $D(x)$ learns to distinguish them from real objects.</p>

                        <img src="images/GAN_scheme.png" alt="Generative Adversarial Net (GAN)"
                             width="60%" style="border-radius: 5%">

                        <hr style="width: 100%; margin: 20px auto;"/>
                        <p style="text-align: left;font-size: smaller">
                            Chris Nicholson. <a href="https://pathmind.com/wiki/generative-adversarial-network-gan">A Beginner's Guide to Generative Adversarial Networks</a>. 2019.</p>
                    </section>
                    <section>
                        <h3>GAN Problem Statement</h3>
                        <p style="text-align: left">There is a sample of objects $\{x_i\}_{i=1}^m$ from $X$.
                            We train</p>
                        <ul>
                            <li>probabilistic generative model $G(z, \alpha): x \sim p(x|z,\alpha)$</li>
                            <li>probabilistic discriminative model $D(x, \beta) = p(x\text{ is real}| x, \beta)$</li>
                        </ul>

                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">Criteria:</p>
                                <ul>
                                    <li>Discriminative model training $D$ (binary cross-entropy in essence):
                                    $$ -L_D = \sum\limits_{i=1}^m \ln D(x_i, {\color{orange}\beta}) + \ln(1 - D(G(z_i, \alpha), {\color{orange}\beta}))
                                        \to {\color{orange}\max\limits_{\beta}}$$
                                    </li>
                                    <li>Learning the generative model $G$ from random noise $\{z_i\}_{i=1}^m$:
                                    $$ L_G = \sum\limits_{i=1}^m \ln(1 - D(G(z_i, {\color{orange}\alpha}), \beta)) \to {\color{orange}\min\limits_{\alpha}}$$
                                    </li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <p style="text-align: left">So the entire task is $ \min\limits_{\alpha, G} \max\limits_{\beta, D} L(\alpha, \beta)$</p>

                        <img src="images/GAN_2014.png" style="border-radius: 5%" width="50%">

                        <p style="text-align: center;font-size: smaller">
                            <a href="https://arxiv.org/pdf/1406.2661.pdf">Ian Goodfellow et al. Generative Adversarial Nets. 2014</a></p>
                    </section>
                    <section>
                        <p class="r-frame">Ok, but how to train it?</p>

                        <div class="fragment">
                            <div class="typesetting">
                        <h3>It is not an easy question!</h3>
                        <p style="text-align: left">Usual SGD "as is" doesn't work here, therefore
                            in first publications authors:</p>
                        <ol style="text-align: left">
                            <li>fix $\alpha$, train $\beta$</li>
                            <li>fix $\beta$, train $\alpha$</li>
                        </ol>
                        <p style="text-align: left">which is similar to EM algorithm.</p>
                            </div>
                        </div>

                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">And this also doesn't work in practice
                                due to gradient vanishing. The first naive workaround is changing</p>
                                $$ L_G = \sum\limits_{i=1}^m \ln(1 - D(G(z_i, {\color{orange}\alpha}), \beta)) \to
                                L_G^\prime = -\sum\limits_{i=1}^m \ln(D(G(z_i, {\color{orange}\alpha}), \beta))$$
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>DCGAN — 2016, Radford et al.</h3>

                        <img src="images/DCGAN.png" style="border-radius: 5%" width="80%">

                        <p style="text-align: center;font-size: smaller">
                            <a href="https://arxiv.org/pdf/1511.06434.pdf">
                                Unsupervised representation learning with DCGAN</a></p>
                    </section>
                    <section>
                        <h3 style="text-align: left">DCGAN feature space</h3>
                        <img src="images/DCGAN_feature_space.png" style="border-radius: 5%" width="80%">
                    </section>
                    <section>
                        <h3>LSGAN — 2017, Mao et al.</h3>

                        <p style="text-align: left">LSGANs is a type of GAN that solves a least squares problem
                            in the process of training a GAN, thus stabilizing the training process.</p>

                        <img src="images/LSGAN_loss_functions.png" alt="LSGAN_loss_functions"
                             style="width: 70%; height: auto;border-radius: 5%">
                        <p style="margin-top: 20px; font-size: 0.9em;">
                            <a href="https://arxiv.org/abs/1611.04076" target="_blank">Source: LSGANs Paper (Arxiv)</a></p>
                    </section>
                    <section>
                        <p style="text-align: left">Mao et al. showed that LSGAN is more robust
                        to architecture changes and less suffer from <b>mode collapse</b></p>
                        <img src="images/LSGAN_results.png" alt="LSGAN_results"
                             style="width: 60%; height: auto;border-radius: 5%">
                        <img src="images/LSGAN_results_2.png" alt="LSGAN_results_2"
                             style="width: 60%; height: auto;border-radius: 5%">
                    </section>
                    <section>
                        <h3>Wasserstein GAN — 2017, Arjovsky et al.</h3>
                        <p style="text-align: left">WGAN is a variant of GANs that uses the Wasserstein distance
                            to measure the difference between the distribution of the data generated
                            by the GAN and the real data.</p>

                        <img src="images/wasserstein-gan-made-easy-1.png" width="60%" style="border-radius: 5%">
                        <p style="margin-top: 20px; font-size: 0.9em;">
                            <a href="http://modelai.gettysburg.edu/2020/wgan/Resources/Lesson4/IntuitiveGuideOT.htm">
                                Source: Mindcodec post</a></p>

                    </section>
                    <section>
                        <p class="r-frame">Wasserstein distance is Earth Mover's Distance (EMD)</p>
                        <div class="fragment">
                            <div class="typesetting">
                                <p>The EMD between probability distributions
                                $P$ and $Q$ can be defined as an infimum over joint probabilities:</p>
                                $$\text{EMD}(P,Q) = \inf\limits_{\gamma \in \Pi(P, Q)}
                                \mathbb{E}_{(x, y) \sim \gamma}\left[d(x, y)\right]\,$$
                                <p style="text-align: left">where $\Pi(P, Q)$ is the set of all
                                    joint distributions whose marginals are $P$ and $Q$.
                                By Kantorovich-Rubinstein duality, this can also be expressed as:</p>
                                $$\text{EMD}(P,Q) = \sup\limits_{\| f \|_L \leq 1} \,
                                \mathbb{E}_{x \sim P}[f(x)] - \mathbb{E}_{y \sim Q}[f(y)]\,$$
                                <p style="text-align: left">where the supremum is taken
                                    over all 1-Lipschitz continuous functions,
                                    i.e. $\| \nabla f(x)\| \leq 1 \quad \forall x$.</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <img src="images/wgan_diagram.png" alt="Diagram of WGAN critic"
                             style="width: 60%; height: auto;border-radius: 5%">
                        <p style="margin-top: 20px; font-size: 0.9em;">
                            <a href="https://arxiv.org/abs/1701.07875" target="_blank">Source: WGANs Paper (Arxiv)</a></p>
                    </section>
                    <section>
                        <p>For evaluation of generated images (or other objects) one usually uses
                        one of two metrics:</p>
                        <ul>
                            <li><a href="https://en.wikipedia.org/wiki/Inception_score">Inseption score</a></li>
                            <li><a href="https://en.wikipedia.org/wiki/Fréchet_inception_distance">
                                FID score — Frecher Inception Distance</a></li>
                        </ul>
                        <br><br>
                        <p>It is almost impossible to use these metrics as losses,
                            since they use a bunch of objects for estimation, not one.</p>
                        <br><br>
                    </section>
                    <section>
                        <h3>StyleGAN Demo</h3>
					<iframe src="https://www.youtube.com/embed/kSLJriaOumA?autoplay=1"
							style="width: 100%;height: 800px">
					</iframe>

                    <p>Papers are here: <a href="https://nvlabs.github.io/stylegan2/versions.html">
                        https://nvlabs.github.io/stylegan2/versions.html</a></p>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Summary</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul style="text-align:left;">
                                    <li>Explored the distinction between
                                        <strong>Generative and Discriminative models</strong> in machine learning,
                                        highlighting their applications, strengths, and limitations.</li>
                                    <li>Discussed <strong>Supervised Learning Task Formulation</strong>
                                        focusing on the probability distributions $p(y|x)$ for discriminative models
                                        and joint distribution $p(x,y)$ for generative models.</li>
                                    <li>Delved into <strong>Generative Models</strong>,
                                        covering Explicit and Implicit Density Models,
                                        including Naive Bayes, Autoregressive Models, Variational Autoencoders (VAE),
                                        and Generative Adversarial Networks (GANs).</li>
                                    <li>Explored <strong>Autoencoders</strong> in depth, including their architecture,
                                        applications, and comparison with PCA.
                                        Also, discussed various types like Sparse and Variational Autoencoders.</li>
                                    <li>Highlighted <strong>GANs</strong> and their development over time,
                                        including DCGAN, LSGAN, and Wasserstein GAN, showcasing their unique features
                                        and improvements in stability and image generation quality.</li>
                                    <li>Reviewed advanced concepts such as the <strong>Wasserstein distance</strong>
                                        and its significance in GANs, alongside practical challenges
                                        and solutions in training GANs.</li>
                                    <li>Concluded with demonstrations of GAN capabilities,
                                        including <strong>StyleGAN</strong>,
                                        and discussed metrics for evaluating generative models
                                        like Inception and FID scores.</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>