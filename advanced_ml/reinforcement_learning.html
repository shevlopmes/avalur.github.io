<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>Reinforcement learning intro</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/black.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
					<div>
						<img src="./images/nup_logo_dark.jpeg" alt="nup_logo" />
					</div>
                    <h2>Advanced machine learning</h2>
					<div class="fragment" style="margin-bottom:20px;">
							<div class="typesetting">
								<h3>Reinforcement learning intro</h3>
								<br />
								Alex Avdiushenko<br />
								March 19, 2024
							</div>
					</div>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Problem Statement</h3>
                        <div class="fragment">
                            <div class="typesetting">
                            <ul>
                                <li>Up to this point, we either restored the function from the training set $(X, Y)$ (supervised learning)
                                    or searched for the structure in the set of objects $X$ (unsupervised learning)</li>
                                <li>But how does learning work in the real world?
                                    Usually we do some action and get the result, gradually learning</li>
                            </ul>
                                <aside class="notes">
                                    Good morning, dear students.
                                    Today we are going to talk about reinforcement learning,
                                    it is a slightly separate topic that is not directly related to deep learning,
                                    but is included in machine learning.
                                    It is quite possible to read a semester course on it,
                                    but we will limit ourselves to a couple of lectures.

                                    Great, let's go on and let me remind you, that up to this point, we found the appropriate function $f$,
                                    based on the training set. And it is called supervised learning.
                                    Or also we found the inner structure in the set of objects, you know, texts or images,
                                    and it is called unsupervised learning, when we haven't answers $y$.

                                    A pretty good illustration of reinforcement learning comes from
                                    the amazing ability of children to learn to walk.

                                    Initially they have muscles and mostly lie down and sleep.
                                    There are no any datasets for supervised or unsupervised learning,
                                    but the whole real physical world is around.
                                    Then they begin to roll over from their stomach to their back and back,
                                    then they learn to crawl, sit, stand, and finally, walk and run.
                                </aside>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Real world</h3>
                        
                        <img src="images/pacman.png" width="1000">

                        <p><a href="https://www.youtube.com/watch?v=QilHGSYbjDQ">Deep Reinforcement Learning in Pacman</a></p>

                        <aside class="notes">
                            Ok, but what do we have in the real world?
                            Usually we do some action and get the result, and so we are learning gradually.
                        </aside>
                    </section>
                    <section>
                        <h3 style="text-align: left">One more motivation</h3>

                        <p>For example, we need to select the main page of a pizzeria website to attract the clients</p>

                        <div class="row">
                            <div class="column">
                                <img style="border-radius: 5%" src="images/pizza-1.png">
                            </div>
                            <div class="column">
                                <img style="border-radius: 5%" src="images/pizza-2.png">
                            </div>
                        </div>

                        <aside class="notes">
                            And another one important example. Suppose you need to choose the most attractive design
                            for the main page of a pizzeria website,
                            but you don’t want to lose a lot of customers, time, money on experiments.
                        </aside>
                    </section>
                    <section>
                        <h3 style="text-align: left">Two fundamental difficulties in RL</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>Exploration vs. Exploitation</li>
                                    <li>Credit assignment</li>
                                </ul>
                            </div>
                        </div>
                        <br>

                        <div class="fragment">
                            <h3 style="text-align: left">What approaches are there?</h3>
                            <div class="typesetting">
                                <ul>
                                    <li style="text-align: left">Multi-armed bandits — a special case of reinforcement learning with only one state,
                                        i.e., without credit assignment difficulty</li>
                                <br>
                                <li style="text-align: left">
                                    Also, there is a statistics approach (i.e. not RL):
                                    A/B testing — many users see potentially bad options;
                                    another problem is that in reality the group size should depend on
                                    the true probability values (e.g., conversions) which you don't know</li>
                                </ul>
                            </div>
                        </div>
                        <aside class="notes">
                            In A/B testing you divide your customers on two groups (A and B),
                            show them different versions of the main page and then measure the results,
                            the conversions and make your conclusions. And what is multi-armed bandits?
                        </aside>
                    </section>
                    <section>
                        <h3>Bernoulli's one-armed bandit</h3>
                        <img src="images/data-kopilkabandit.jpg" alt="Bandit" style="width: 30%;border-radius: 5%">
                        <p>Probability of winning $\theta = 0.05$</p>
                        <aside class="notes">
                            Let's begin from the one-armed bandit. So you pull the handle of
                            a slot machine, and you can win with some probability theta.
                        </aside>
                    </section>
                    <section>
                        <h3>Multi-Armed Bandits</h3>
                        <div style="display: flex; justify-content: space-between;">
                            <div>
                                <img src="images/data-kopilkabandit.jpg" alt="Bandit" style="width: 80%;border-radius: 5%">
                                <p>$\theta_1 = 0.02$</p>
                            </div>
                            <div>
                                <img src="images/data-kopilkabandit.jpg" alt="Bandit" style="width: 80%;border-radius: 5%">
                                <p>$\theta_2 = 0.01 (min)$</p>
                            </div>
                            <div>
                               <img src="images/data-kopilkabandit.jpg" alt="Bandit" style="width: 80%;border-radius: 5%">
                                <p>$\theta_3 = 0.05$</p>
                            </div>
                            <div>
                                <img src="images/data-kopilkabandit.jpg" alt="Bandit" style="width: 80%;border-radius: 5%">
                                <p>$\theta_4 = 0.1 (max)$</p>
                            </div>
                        </div>
                        <p>We do not know the true probabilities, but we want to come up with a strategy that maximizes the payoff (reward)</p>
                        <aside class="notes">
                            In multi-armed bandits model you have several handles with some winning probabilities theta one, two and so on.
                        </aside>
                    </section>
                    <section>
                        <h3>Mathematical statement of the problem</h3>
                        <p style="text-align: left">Given possible actions $x_1, \dots, x_n$</p>
                        <p>At the next iteration $t$, for each action $x^t_i$ performed,
                            we get the answer</p>
                        $$ y^t_i \sim q(y|x^t_i, \Theta),$$
                        <p style="text-align: left">which brings us a <strong>reward</strong></p>
                        $$r_i^t = r(y^t_i)$$
                        <p style="text-align: left">There is an optimal action $x_{i^*}$ (sometimes $x_{i^*_t}$)</p>
                        $$\forall i:\ \mathbb{E}(r_{i^*_t}^t) \geq \mathbb{E}(r^t_i) $$

                        <div class="fragment">
                            <div class="typesetting">
                                <p class="r-frame"><b>Question:</b> How to evaluate different strategies?</p>
                            </div>
                        </div>
                        <aside class="notes">
                            Ok, now let's move on, to a mathematical statement of the problem.
                        </aside>
                    </section>
                    <section>
                        <h3>Measure of quality</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">The quality measure of the multi-armed bandit algorithm $a$ is usually <b>regret</b></p>
                                $$ R(a) = \sum\limits_{t=1}^T \left(\mathbb{E}(r_{i^*_t}^t) - \mathbb{E}(r_{i^a_t}^t)\right)$$
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">Under synthetic conditions (when we know the probabilities), we can consider</p>
                                $$ E\left( R \right) = \int\limits_{\Theta} R(a)d\Theta $$
                            </div>
                        </div>
                    </section>
                    <section>
                        <img src="images/regret_plot.png" style="width: 80%;border-radius: 5%">
                    </section>
                    <section>
                        <h3>Multi-Armed Bandits possible applications</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <div style="display: flex; justify-content: space-between;">
                                    <div>
                                        <h4>Areas:</h4>
                                        <ul>
                                            <li>advertising banners</li>
                                            <li>recommendations (goods, music, movies etc.)</li>
                                            <li>real slot machines in the casino</li>
                                        </ul>
                                    </div>
                                    <div>
                                        <h4>Approaches:</h4>
                                        <ul>
                                            <li>Thompson sampling</li>
                                            <li>Upper Confidence Bound (UCB)</li>
                                            <li>$\varepsilon$-greedy strategy</li>
                                        </ul>
                                    </div>
                                </div>
                            </div>
                        </div>
                        <aside class="notes">
                            Ok, and what are the possible applications of multi-armed bandits?
                            And there are models of implementing and optimizing the multi-armed bandits,
                            but we won't discuss them in detail here.

                            The main difference from more general reinforcement learning approach is that
                            there is no environment state in multi-armed bandits.
                        </aside>
                    </section>
                    <section>
                        <h3>Multi-Armed Bandits Disadvantage</h3>
                        <p style="text-align: left">Do not take into account delayed effects. For example, the effect of clickbait in advertising</p>

                        <img src="images/cats_invaders.jpg" alt="Cats" style="width: 45%;border-radius: 5%">
                        <p>SHOCK! Cats want to enslave us</p>
                        <aside class="notes">
                            So you may increase your metrics for some short period of time by clickbait,
                            but in the long run you will lose clients and money consequently.
                        </aside>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Reinforcement Learning Examples</h3>
                                <div style="display: flex; justify-content: space-evenly">
                                    <img src="images/rl_scheme.png" alt="RL Scheme" style="width: 60%;border-radius: 5%">
                                    <div class="fragment">
                                        <div class="typesetting">
                                            <br><br>
                                            <ul>
                                                <li>robot control</li>
                                                <li>(video) games</li>
                                                <li>security management</li>
                                            </ul>
                                        </div>
                                    </div>
                                </div>
                        <aside class="notes">
                            Let's go on to the Reinforcement Learning model.
                        </aside>
                    </section>
                    <section>
                        <h3>Agent Model in a Changing Environment</h3>
                        <div style="display: flex; justify-content: space-between;">
                            <div>
                                <h3 style="text-align: left">Definitions:</h3>
                                <ul>
                                    <li>state $s \in S$</li>
                                    <li>agent action $a \in A$</li>
                                    <li>reward $r \in R$</li>
                                    <li>state transition dynamics $P(s_{t+1} | s_t, a_t, \dots, s_{t-i}, a_{t-i}, \dots, s_0, a_0)$</li>
                                    <li>win function $$r_t = r(s_t, a_t, \dots, s_0, a_0)$$</li>
                                    <li>total reward $R = \sum\limits_t r_t$</li>
                                    <li>agent policy $\pi (a | s)$</li>
                                </ul>
                            </div>
                            <div>
                                <h3 style="text-align: left">Task:</h3>
                                $$ \pi (a | s): \mathbb{E}_\pi [R] \to \max $$
                            </div>
                        </div>
                        <aside class="notes">
                            So, more formally you have states, actions and rewards.
                            And your task is to find optimal policy to maximize the total reward.
                        </aside>
                    </section>
                    <section>
                        <h3>Cross-entropy method. Algorithm</h3>
                        <p style="text-align: left">Trajectory — $[s_0, a_0, s_1, a_1, s_2, \dots, a_{T-1}, s_T]$</p>
                        <div class="fragment">
                            <div class="typesetting">
                            <ul>
                                <li>Initialize strategy model $\pi(a | s)$</li>
                                <li>Repeat:
                                    <ul>
                                        <li>play $N$ sessions</li>
                                        <li>choose the best $K$ of them and take their trajectories</li>
                                        <li>adjust $\pi(a | s)$ so that strategy is able to maximize
                                            the probabilities of actions from the best trajectories</li>
                                    </ul>
                                </li>
                            </ul>
                            </div>
                        </div>
                        <aside class="notes">
                            The first simple algorithm to find appropriate policy is called cross-entropy method.
                            Here you have some initialization, maybe random, as usual, then you play $N$ sessions,
                            choose best of them by your reward and update your strategy.
                        </aside>
                    </section>
                    <section>
                        <h3>Cross-entropy method. Implementation with a table</h3>
                        <p style="text-align: left">As a strategy model, we simply take a matrix $\pi$ of dimension $|S| \times |A|$</p>
                        $$\pi(a | s) = \pi_{s,a}$$

                        <p style="text-align: left">after selecting the best trajectories, we obtain a set of pairs</p>
                        $$\text{Elite} = [(s_0, a_0), (s_1, a_1), (s_2, a_2), \dots, (s_H, a_H)]$$

                        <p style="text-align: left">and maximize the likelihood</p>
                        $$ \pi_{s,a} = \frac{\sum\limits_{s_t, a_t \in \text{Elite}}
                        [s_t = s][a_t = a]}{\sum\limits_{s_t, a_t \in \text{Elite}} [s_t = s]}$$
                    </section>
                    <section>
                        <h3>Training Example</h3>
                        <img src="images/cartpole_cmp.gif" alt="Training" style="width: 70%;border-radius: 5%">
                    </section>
                    <section>
                        <h3>There is a problem..</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">That there are a lot of states:</p>
                                <img src="images/starcraft3.jpg" alt="Starcraft" style="width: 80%;border-radius: 5%">
                            </div>
                        </div>

                        <aside class="notes">
                            So the problem is that we cannot even store in memory the whole table of states.
                        </aside>
                    </section>
                    <section>
                        <h3>Approximate cross-entropy methods</h3>
                        <p style="text-align: left">Possible solutions:</p>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>Split the state space into sections and treat them as states</li>
                                    <li>Get probabilities from $\pi_\theta (a | s)$ machine learning model:
                                        linear model, neural network, random forest
                                        (often these probabilities need to be further specified later)</li>
                                </ul>
                            </div>
                        </div>
                        <aside class="notes">
                          So people invented Approximate cross-entropy methods.
                        </aside>
                    </section>
                    <section>
                        <h3>Example</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>As a strategy model, we take just a neural network $\pi_\theta$</li>
                                    <li>Initialize with random weights</li>
                                    <li>At each iteration, after selecting the best trajectories,
                                        we obtain a set of pairs $$\text{Elite} = [(s_0, a_0), (s_1, a_1), (s_2, a_2), \dots, (s_H, a_H )]$$</li>
                                    <li>And perform optimization
                                        $$ \pi = \arg\max\limits_\theta \sum\limits_{s_i, a_i \in \text{Elite}} \log \pi(a_i|s_i) = \arg\max\limits_\theta \mathscr{ L}(\theta)$$</li>
                                    <li>That is $$ \theta_{t+1} = \theta_{t} + \alpha \nabla \mathscr{L}(\theta) $$</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Disadvantages of the cross-entropy method</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>It is unstable for small samples</li>
                                    <li>In the case of a non-deterministic environment,
                                        it chooses lucky cases (randomly played in favor of the agent)</li>
                                    <li>It is focusing on behavior in simple states</li>
                                    <li>It ignores a lot of information</li>
                                    <li>There are tasks in which the end never comes (stock market game, for instance)</li>
                                </ul>
                            </div>
                        </div>
                        <aside class="notes">
                            We need to correct these disadvantages.
                        </aside>
                    </section>
                    <section>
                        <h3>Markov Decision Process</h3>
                        <div style="display: flex; justify-content: space-between;">
                            <div>
                                <h4>Definitions:</h4>
                                <ul>
                                    <li>state $s \in S$</li>
                                    <li>agent action $a \in A$</li>
                                    <li>reward $r \in R$</li>
                                    <li>transition dynamics (Markov's assumption)
                                        $$ P(s_{t+1} | s_t, a_t, \dots, s_{t-i}, a_{t-i}, \dots, s_0, a_0) = P(s_{t+1} | s_t, a_t)$$</li>
                                    <li>win function (Markov's assumption)
                                        $$r_{t} = r(s_t, a_t)$$</li>
                                    <li>total reward $R = \sum\limits_t r_t$</li>
                                </ul>
                            </div>
                            <div>
                                <h4>Task:</h4>
                                <ul>
                                    <li>agent policy $ \pi (a | s) $</li>
                                </ul>
                                $$ \pi (a | s): \mathbb{E}_\pi [R] \to \max $$
                            </div>
                        </div>
                        <aside class="notes">
                            Here we have Markov's assumption, which is actually not so strong, as we could expect.
                            For example, it is true in chess and Starcraft also.
                            Only your current state determines everything, and not the history of the game.
                        </aside>
                    </section>
                    <section>
                        <h3>Important definitions</h3>
                        <div class="typesetting">
                            <div class="fragment">
                                <p style="text-align: left">Average absolute win:
                                $$ \mathbb{E}_{s_0 \sim P(s_0)\,} \mathbb{E}_{a_0 \sim \pi(a|s_0)\,} \mathbb{E}_{s_1, r_0 \sim P(s^\prime,r|s, a)} \dots \left[r_0 + r_1 + \dots + r_T \right]$$</p>
                            </div>
                            <div class="fragment">
                                <p style="text-align: left">On-Policy Value Function:
                                $$ V_{\pi} (s) = \mathbb{E}_\pi [R_t|s_t = s] = \mathbb{E}_\pi \left[\sum\limits_{k=0}^\infty \gamma^k r_{k+t+1} | s_t = s\right]$$</p>
                            </div>
                            <div class="fragment">
                                <p style="text-align: left">On-Policy Action-Value Function:
                                $$ Q_{\pi} (s, a) = \mathbb{E}_\pi [R_t|s_t = s, a_t = a] = \mathbb{E}_\pi \left[\sum\limits_{k=0}^\infty \gamma^k r_{k+t+1} | s_t = s, a_t = a \right]$$</p>
                                <p style="text-align: left">where $\pi$ is the strategy followed by the agent, $\gamma \in [0, 1]$</p>
                            </div>
                        </div>
                        <aside class="notes">
                            Why would we ever want a discount factor, though?
                            Don't we just want to get all the rewards? We do, but the discount factor is both intuitively appealing and mathematically convenient.
                            On an intuitive level: cash now is better than cash later.
                            Mathematically: an infinite-horizon sum of rewards may not converge to a finite value,
                             and it is hard to deal with in equations. But with a discount factor and under reasonable conditions, the infinite sum converges.
                        </aside>
                    </section>
                    <section>
                        <p class="r-frame">Question: What is the meaning of the difference $Q_{\pi} (s, a) - V_{\pi} (s)$?</p>
                        <br><br>
                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">
                                    Often in RL, we don't need to describe how good an action is in an absolute sense,
                                    but only how much better it is than others on average.
                                    We make this concept precise with the <b>advantage function</b>:
                                    $$ A_{\pi} (s, a) = Q_{\pi} (s, a) - V_{\pi} (s) $$
                                </p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Bellman Equation</h3>
                        <p style="text-align: left">For the Action-Value function $Q$</p>
                        $$ Q_{\pi} (s,a) =
                        \mathbb{E}_\pi \left[r_t + \gamma \sum\limits_{a^\prime} \pi(a^\prime|s^\prime)
                        Q_{\pi}(s^\prime,a^\prime) \mid s_t = s, a_t = a \right]$$
                    </section>
                    <section>
                        <h3 style="text-align: left">State utility recalculation</h3>
                        $$V(s) = \max\limits_a [r(s, a) + \gamma V(s^\prime (s, a))]$$

                        <p style="text-align: left">That is, with probabilistic transitions</p>
                        $$V(s) = \max\limits_a [r(s, a) + \gamma \mathbb{E}_{s^\prime \sim P(s^\prime | s, a)} V(s^ \prime)]$$

                        <p style="text-align: left">Iterative State Utility Recalculation Formula:</p>
                        $$
                        \begin{align*}
                          \forall s \ V_0(s) &= 0 \\
                          V_{i+1}(s) &= \max\limits_a [r(s, a) + \gamma \mathbb{E}_{s^\prime \sim P(s^\prime | s, a)} V_{i}(s^\prime)]
                        \end{align*}
                        $$

                        <p class="r-frame">Note: To use this formula in practice,
                            we need to know the transition probabilities $P(s^\prime| s, a)$</p>
                    </section>
                    <section>
                        <h3 style="text-align: left">Utility of an action</h3>
                        $$Q(s, a) = r(s, a) + \gamma V(s^\prime)$$
                        <p style="text-align: left">The strategy of the game is defined as follows</p>
                        $$\pi(s) : \arg\max\limits_a Q(s, a)$$
                        <p style="text-align: left">Again due to stochasticity</p>
                        $$Q(s, a) = \mathbb{E}_{s^\prime} [r(s, a) + \gamma V(s^\prime)]$$

                        <p style="text-align: left">It is possible to estimate the expected value without an explicit distribution,
                            using the Monte Carlo method and averaging over the outcomes:</p>
                        $$Q(s_t, a_t) \leftarrow \alpha \left(r_t+\gamma \max\limits_a Q(s_{t+1}, a) \right)
                        + (1-\alpha) Q(s_{t}, a_{t})$$
                    </section>
                    <section>
                        <p style="text-align: left">In theory, we can solve SLAE of Bellman Equations, get $Q^*(s,a)$ and that's it.</p>

                        <div class="fragment">
                            <div class="typesetting">
                                <p style="text-align: left">In practice, we have lots of problems:</p>
                                <ul>
                                    <li>how to enumerate all possible strategies $\pi$?</li>
                                    <li>usually we don't know $r_t$ and probability distributions to calculate expectation</li>
                                    <li>also, the amount of all state is really huge and uncountable</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <p style="text-align: left">Let's consider <b>optimal</b> value function:</p>
                        $$Q^*(s,a) = \max\limits_\pi \mathbb{E}_\pi [R_t|s_t = s, a_t = a] $$
                        <br>
                        <p style="text-align: left">We have similar Bellman Equation for $Q^*(s,a)$ and if we solved it,
                            our strategy would be obvious:
                        $$\pi^*(s) = \arg\max\limits_a Q^*(s,a)$$
                        <br>
                        <div class="fragment">
                            <div class="typesetting">
                            <p class="r-frame" style="text-align: left">
                                $\quad$ The greedy strategy $\pi$ with respect to a solution of Bellman Equations $Q^*(s,a)$ (SLAE)
                                "choose the action that maximizes $Q^*$" is optimal.</p>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Temporal Difference training (TD)</h3>
                        $$ V_{\pi} (s) = \mathbb{E}_\pi [R_t|s_t = s] =
                        \mathbb{E}_\pi \left[r_{t+1} + \gamma R_{t+1} | s_t = s\right] \\
                        \Rightarrow V_{\pi} (s_t) \sim R_{t+1} + \gamma V_{\pi} (s_{t+1}) $$

                        <div class="fragment">
                            <div class="typesetting">
                                <p>We arbitrarily initialise the function $V_{\pi}(s)$ and the strategy $\pi$. Then we repeat:</p>
                                <ul>
                                    <li>Initialise $s$</li>
                                    <li>For each agent step:
                                        <ul>
                                            <li>Select $a$ by strategy $\pi$</li>
                                            <li>Do action $a$, get result $r$ and next state $s^\prime$</li>
                                            <li>Update function $V(s)$ by formula
                                                $V(s) = V(s) + \alpha \left[r + \gamma V(s^\prime) - V(s)\right]$</li>
                                            <li>Go to the next step by assigning $s := s^\prime$</li>
                                        </ul>
                                    </li>
                                </ul>
                            </div>
                        </div>
                        <aside class="notes">
                            The amazing fact is, that temporal difference training works.
                            The information about states' value flows "from right to left".
                        </aside>
                    </section>
                    <section>
                        <h3>SARSA and Q-Learning in Temporal Difference (TD) Training</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                <h4 style="text-align: left">SARSA</h4>
                                <p style="text-align: left">State-Action-Reward-State-Action computes the Q-value based
                                    on the current state of the policy and is considered as an on-policy learning algorithm.
                                    Named by the quintuple (s,a,r,s',a'), the formula is as follows:</p>
                                $$ Q(s,a) \leftarrow Q(s,a) + \alpha \left[r + \gamma Q(s',a') - Q(s,a)\right] $$
                            </div>
                        </div>

                        <div class="fragment">
                            <div class="typesetting">
                                <h4 style="text-align: left">Q-Learning</h4>
                                <p style="text-align: left">Q-Learning is an off-policy learning algorithm that computes
                                    the Q-value based on the maximum reward that is attainable in the next state.
                                    The formula is as follows:</p>
                                $$ Q(s,a) \leftarrow Q(s,a) + \alpha \left[r + \gamma \max_{a}\left[Q(s',a')\right] - Q(s,a)\right] $$
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>DQN (Deep Q-Learning Network)</h3>
                        <p style="text-align: left">Environment is Atari game emulator, and each frame is
                            $210\times 160\text{ pix}, \ 128\text{ colors}$</p>
                        <p><img src="images/atari_games.png" style="width: 100%;" alt="Atari Games"/></p>

                            <div style="display: flex; justify-content: space-between;width: 80%;padding-left: 220px">
                                <p>Pong</p>
                                <p>Breakout</p>
                                <p>Space Invaders</p>
                                <p>Seaquest</p>
                                <p>Beam Rider</p>
                            </div>

                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>States <b>s</b>: 4 consecutive frames compressed to $84 \times 84$</li>
                                    <li>Actions <b>a</b>: from 4 to 18, depending on the game</li>
                                    <li>Rewards <b>r</b>: current in-game SCORE</li>
                                    <li>Value function $Q(s, a; w)$: ConvNN with input $s$ and $|A|$ outputs</li>
                                </ul>

                                <p>Source: V.Mnih et al. (DeepMind). Playing Atari with deep reinforcement learning. 2013</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>DQN Method</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>Saves trajectories $(s_t, a_t, r_t)_{t=1}^T$ in <b>replay memory</b>
                                        for repeated experience replay. It is needed to prevent overfitting</li>
                                    <li>Gets approximation of the optimal value function $Q(s_t, a_t)$
                                        for fixed current network parameters ${\color{orange} w_t}$</li>
                                    <li>Evaluates loss function for training the neural network model $Q(s, a; w)$:
                                    $$ \mathscr{L} (w) = (Q(s_t, a_t; w) - y_t)^2 $$
                                    with stochastic gradient SGD (by mini-batches of length 32):
                                    $$ w_{t+1} = w_{t} - \eta \left(Q(s_t, a_t; w_t) - y_t\right) \nabla_w Q(s_t, a_t; w_t) $$</li>
                                    <li>Also it is better to use two different models for max evaluation and updating
                                        (Double Q-learnig by Hado van Hasselt, 2010)</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Network architecture for the value function</h3>
                        <p>
                            <img src="images/dql-cnn-arch.png" style="width:85%;border-radius: 5%" alt="DQL CNN Architecture"/>
                        </p>
                    </section>
                    <section data-background-image="images/alpha_go.png">
                    </section>
                    <section data-background-image="images/AI-AlphaStar-StarCraft-II.png">
                    </section>
                </section>
                    <section>
                        <h3>Dueling Networks</h3>

                        <p style="text-align: left">Dueling Network is a reinforcement learning architecture.
                            The idea behind Dueling Networks is that in many settings,
                            it could be beneficial to separate the representation of state values and
                            the advantages of each action in a given state.</p>

                        <br>
                        <div class="fragment">
                            <div class="typesetting">
                                <h4 style="text-align: left">Architecture</h4>

                                <p style="text-align: left">The architecture of Dueling Networks consists of two streams.
                                    One stream is the value function stream, which estimates the value function $V(s)$;
                                    the other is the advantage function stream, which estimates advantage function $A(s, a)$.
                                    These two streams are combined to produce the estimate of $Q(s, a)$.</p>
                            </div>
                        </div>

                        <aside class="notes">
                            The advantage of Dueling Networks is that it can learn which states are (or are not) valuable,
                            without having to learn the effect of each action at every state.
                            This is particularly appealing for states with no changes, or for states where
                            the effect of certain actions does not vary a lot.

                            The Dueling Network architecture was introduced in a paper titled
                            "Dueling Network Architectures for Deep Reinforcement Learning"
                            by Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas,
                            and it has been widely used in the Deep Reinforcement Learning field since its introduction.
                        </aside>

                    </section>
                <section>
                    <section data-background-color="white">
                        <h3>RL recap</h3>
                        <img src="images/rl_recap.png" width="80%" style="border-radius: 5%">
                    </section>
                    <section data-background-color="white">
                        <img src="images/mcmc_barto.png" width="50%" style="border-radius: 5%"><br>
                        <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Source: BartoSutton.pdf</a>
                    </section>
                    <section>
                        <h3>Policy gradient</h3>
                        <p style="text-align: left">We are trying to train just policy</p>
                            $$\pi(a|s, \theta) = Pr[a_t = a| s_t = s]$$

                        <div class="fragment">
                            <div class="typesetting">
                                <ol>
                                    <li>We can train stochastic policies, where $\pi(a|s) \neq 0,1$</li>
                                    <li>We even can train raw scores only, applying Softmax for moves probabilities</li>
                                </ol>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Policy gradient theorem</h3>

                        <div class="fragment">
                            <div class="typesetting">
                                $$
                                \theta_{t+1} = \theta_{t} + \alpha \nabla_\theta J(\theta_t) \\
                                J(\theta) = V_{\pi_\theta}(s_0) \underset{\theta}{\to} \max \\
                                \boxed{\nabla_\theta J(\theta) \propto \sum\limits_s Pr_\pi[s]
                                \sum\limits_a Q_\pi(s,a) \nabla_\theta \pi(a|s)}
                                $$
                                <p style="text-align: left">
                                    Probabilities $Pr_\pi[s]$ are very hard to calculate,
                                    but we can just play using policy $\pi$, and this gives us
                                    correct samples from this distribution.</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="typesetting">
                            <p style="text-align: left">The REINFORCE algorithm is a policy-based method used in reinforcement learning.
                                Unlike value-based methods, which aim to learn a value function by interaction with
                                the environment, REINFORCE learns a stochastic policy directly.</p>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Summary</h3>
                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                    <li>We got acquainted with the reinforcement learning problem statement</li>
                                    <li>Touched a little multi-armed bandits model</li>
                                    <li>Got acquainted with the methods of cross-entropy, TD-learning and Q-learning</li>
                                    <li>Discussed Deep Q-learning Network</li>
                                    <li>We finished with policy gradient descent (the central method in robotics)</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment">
                            <div class="typesetting">
                                <br>
                                <h3 style="text-align: left">What else can you see?</h3>
                                <ul>
                                    <li><a href="https://spinningup.openai.com/en/latest">Educational resource on RL produced by OpenAI</a></li>
                                    <li>The main book on the topic:
                                        <a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf">Reinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto</a></li>
                                    <li>Yuxi Li. Resources for Deep Reinforcement Learning. 2018</li>
                                    <li><a href="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver">Intro to RL with David Silver, DeepMind × UCL, 2015</a></li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>