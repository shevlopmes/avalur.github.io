<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/html" xmlns="http://www.w3.org/1999/html">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>From LLM to RAG and agents</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/black.css">

        <!-- Theme used for syntax-highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
			.important {
				color: orange;
			}
			ul.custom-list {
				list-style: none;
				padding: 0;
			}
			ul.custom-list li::before {
				content: '+';
				margin-right: 8px;
                color: green;
                font-size: larger;
			}
			ul.custom-list li.minus::before {
				content: '-';
                color: red;
                font-size: larger;
			}
			ul.custom-list li.lamp::before {
				content: 'ðŸ’¡';
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
                    <section>
                        <div>
                            <img src="./images/nup_logo_dark.jpeg" alt="nup_logo" />
                        </div>
                        <h2>Advanced machine learning</h2>
                        <div class="fragment" style="margin-bottom:20px;">
                                <div class="typesetting">
                                    <h3>From LLM to RAG and agents</h3>
                                    <br />
                                    Alex Avdiushenko<br />
                                    April 30, 2025
                                </div>
                        </div>
                    </section>
                    <section>
                      <h3 style="text-align: left">Lecture Plan</h3>
                      <ol>
                        <li>Retrieval-Augmented Generation
                            <ul>
                                <li>Vector DB-based</li>
                                <li>Knowledge Graph-based</li>
                                <li>Knowledge Map-based</li>
                            </ul>
                        </li>
                        <li>Using Long Contexts and SSMs</li>
                        <li>Agents</li>
                      </ol>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <img src="images/LLM_FT.png" alt="LLM_FT.png">
                    </section>
                    <section data-background-color="white">
                      <h3 style="text-align: left;">Problems</h3>
                      <ul style="text-align: left;">
                        <li><strong>Accuracy issues:</strong></li>
                          <ul>
                            <li><strong class="important">Knowledge cutoffs:</strong>
                                parameters are usually only updated to a particular time</li>
                            <li><strong class="important">Private data:</strong> data stored in private text or
                                data repositories not suitable for training</li>
                            <li><strong class="important">Learning failures:</strong>
                                even for data that the model was trained on,
                                it might not be enough to get the right answer</li>
                          </ul>
                        <li><strong class="important">Verifiability issues:</strong>
                            It is hard to tell if the answer is correct</li>
                      </ul>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">OpenAI's pricing</h3>
                        <img src="images/GPT4o_pricing.png" alt="GPT4o_pricing.png" style="border-radius: 5%">
                    </section>
                    <section data-background-color="white">
                          <p style="text-align: left">3 Ways to Improve Your LLM</p>
                          <img src="images/improving-llms.png" alt="improving-llms" width="70%">
                            <p>Source: <a href="https://www.maartengrootendorst.com/blog/improving-llms/">
                            https://www.maartengrootendorst.com/blog/improving-llms/</a></p>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left"><a href="https://www.griddynamics.com/blog/retrieval-augmented-generation-llm">
                            RAG process flow
                        </a></h3>
                        <img src="images/RAG_process.png" alt="RAG_process" width="70%">
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Document splitting</h3>
                        <img src="images/doc_split.png" alt="doc_split">
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Semantic proximity of sentences</h3>
                        <img src="images/semantic_proximity.png" alt="semantic_proximity" width="70%">
                    </section>
                    <section data-background-color="white">
                      <h3 style="text-align: left;">Vector DBs</h3>
                      <ul style="text-align: left;">
                        <li><a href="https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/">https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/</a></li>
                        <li><a href="https://github.com/microsoft/SPTAG">https://github.com/microsoft/SPTAG</a></li>
                        <li><a href="https://milvus.io/">https://milvus.io/</a></li>
                        <li><a href="https://www.trychroma.com/">https://www.trychroma.com/</a></li>
                        <li><a href="https://weaviate.io/">https://weaviate.io/</a></li>
                        <li><a href="https://www.elastic.co/">https://www.elastic.co/</a></li>
                        <li><a href="https://www.pinecone.io/">https://www.pinecone.io/</a></li>
                      </ul>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Document retrieval</h3>
                        <img src="images/doc_retrieval.png" alt="doc_retrieval" width="70%">
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Answer generation</h3>
                        <img src="images/answer_gen.png" alt="answer_gen" width="70%">
                    </section>
                    <section data-background-color="white">
                        <img src="images/intro_to_inf_ret.png" alt="intro_to_inf_ret" width="80%">
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">RAG Survey:
                            <a href="https://arxiv.org/pdf/2312.10997">[Gao et al. 2023]</a></h3>
                        <img src="images/RAG_survey.png" alt="RAG_survey">
                    </section>
                    <section data-background-color="white">
                        <img src="images/RAGs_scheme.png" alt="RAGs_scheme">
                    </section>
                    <section data-background-color="white">
                        <img src="images/RAGvsFT.png" alt="RAGvsFT" width="70%">
                    </section>
                    <section data-background-color="white">
                        <img src="images/RAG_ecosys.png" alt="RAG_ecosys" width="70%">
                    </section>
                    <section data-background-color="white">
                      <h3 style="text-align: left;">When do we Retrieve?</h3>
                      <ul style="text-align: left;">
                        <li class="fragment"><strong>Once, at the beginning of generation</strong>
                          <ul>
                            <li>Default method used by most systems
                                <a href="https://proceedings.neurips.cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf">
                                    [Lewis et al. 2020]</a></li>
                          </ul>
                        </li>
                        <li class="fragment"><strong>Several times during generation, as necessary</strong>
                          <ul>
                            <li>Generate a search token (Toolformer) <a href="https://arxiv.org/abs/2302.04761">[Schick et al. 2023]</a></li>
                            <li>Search when the model is uncertain (Active RAG) <a href="https://arxiv.org/abs/2305.06983">[Jiang et al. 2023]</a></li>
                          </ul>
                        </li>
                        <li class="fragment"><strong>Every token</strong>
                          <ul>
                            <li>Find similar final embeddings <a href="https://arxiv.org/abs/1911.00172">[Khandelwal et al. 2019]</a></li>
                            <li>Approximate attention with nearest neighbors (Unlimiformer) <a href="https://arxiv.org/abs/2305.01625">[Bertsch et al. 2023]</a></li>
                          </ul>
                        </li>
                      </ul>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Effectively Using Long Contexts</h3>
                        <img src="images/long-context-recall-pressure-test-batch-2-v0-1kvlhgerh0bc1.webp"
                             alt="long context" width="85%">
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">The State Space Model (SSM)</h3>
                        <p style="text-align: left">The continuous state space model (SSM) is
                            a fundamental representation defined by two simple equations:</p>
                            $$
                            \begin{align*}
                                x'(t) &= A x(t) + B u(t)\\
                                y(t) &= C x(t) + D u(t)
                            \end{align*}
                            $$
                        <p style="text-align: left">
                            Here the input \(u(t)\) is a 1-dimensional signal,
                            the state \(x(t)\) is a $N$-dimensional latent representation satisfying a linear ODE,
                            and the output \(y(t)\) is a simple 1-dimensional projection of the state.
                        </p>
                        <p class="fragment" style="text-align: left">
                            Here \( A \in \mathbb{R}^{N \times N} \) is called the state matrix,
                            and the other parameter shapes are \( B \in \mathbb{R}^{N \times 1},
                            C \in \mathbb{R}^{1 \times N}, D \in \mathbb{R}^{1 \times 1} \).
                        </p>
                    </section>
                    <section data-background-color="white">
                        <ul>
                            <li>The SSM is a fundamental representation used in many scientific and engineering disciplines</li>
                            <li>Conventionally in these areas, the system parameters \( A, B, C, D \) are assumed to be latent (i.e. fixed)</li>
                            <li>We simply use the state space as a black box representation in the spirit of deep learning,
                            where we view an SSM as a function-to-function map parameterized by parameters \( A, B, C, D \)</li>
                        </ul>
                        <p style="text-align: left"><a href="https://hazyresearch.stanford.edu/blog/2022-01-14-s4-3">
                            [Stanford Hazy Research 2022]
                        </a></p>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">Three Representations of SSM</h3>
                        <img src="images/SSMs_representations.png" alt="SSMs_representations.png" width="60%">
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Mamba: Selective State Spaces</h3>
                        <div class="row">
                            <div class="column">
                                <ul>
                                    <li>Many flavors of SSMs have been successful in domains
                                        involving continuous signal data such as audio and vision,
                                        but they have been <strong class="important">less effective
                                            at modeling discrete and information-dense data</strong> such as text</li>
                                    <li>Authors propose a new class of selective state space models that achieve
                                        the modeling power of Transformers while scaling linearly in sequence length</li>
                                    <li>We need the ability to focus on or ignore particular inputs,
                                        <strong class="important">selectivity</strong></li>
                                </ul>
                            </div>
                            <div class="column">
                                <img src="images/snake_robot.png" alt="Mamba Snake and Robot Illustration" style="width: 100%;">
                            </div>
                        </div>
                        <p style="text-align: left">
                            <a href="https://arxiv.org/abs/2312.00752">
                                Mamba: Linear-Time Sequence Modeling with SSS</a>,
                            <a href="https://t.me/gonzo_ML/2148">Gonzo ML (in Russian)</a>
                        </p>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Jamba: SSM-Transformer Hybrid</h3>
                        <div class="row">
                            <div class="column">
                                <ul>
                                    <li>The first production-grade Mamba-based open model (Apache 2.0)</li>
                                    <li>Joint Attention and Mamba (Jamba) architecture.
                                        Composed of Transformer, Mamba, and mixture-of-experts (MoE) layers</li>
                                    <li><strong class="important">12B active weights of 52B MoE weights</strong>, 256K context window</li>
                                    <li>Low memory footprint. Fits 140K context on a single A100 80 GB GPU</li>
                                    <li>High throughput, competitive quality</li>
                                </ul>
                            </div>
                            <div class="column" style="width: 40%; text-align: right;">
                                <img src="images/Jamba_architecture.png" alt="Jamba Architecture Diagram" style="width: 100%;">
                            </div>
                        </div>
                        <p style="text-align: left">
                            <a href="https://www.ai21.com/blog/announcing-jamba">https://www.ai21.com/blog/announcing-jamba</a>
                        </p>
                    </section>
                    <section data-background-color="white">
                        <img src="images/Jamba_context_window.png" alt="Jamba_context_window">
                        <p style="text-align: left">
                            <a href="https://www.ai21.com/blog/announcing-jamba-model-family">
                                https://www.ai21.com/blog/announcing-jamba-model-family</a>
                        </p>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">
                            Unifying Large Language Models and Knowledge Graphs: A Roadmap</h3>
                        <p style="text-align: left">Shirui Pan, <em>Senior Member, IEEE</em>,
                            Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu, <em>Fellow, IEEE</em></p>
                        <p style="text-align: left">
                            <a href="https://arxiv.org/pdf/2306.08302.pdf">https://arxiv.org/pdf/2306.08302.pdf</a>
                        </p>
                        <img src="images/KG_LLMs.png" alt="KG_LLMs">
                    </section>
                    <section data-background-color="white">
                        <img src="images/KG_examples.png" alt="KG_examples">
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Categorization of research on KGs</h3>
                        <img src="images/KG_research.png" alt="KG_research" width="70%">
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Fine-Tuning with Knowledge Graph</h3>
                        <img src="images/FT_with_KG.png" alt="FT_with_KG" width="60%">
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">AI-Powered Knowledge Maps for Navigating Unstructured Data</h3>
                        <a href="https://medium.com/kineviz/ai-powered-knowledge-maps-for-navigating-unstructured-data-97a55be43ed3">
                            <img src="images/KMap.webp" alt="Knowledge_Map" width="55%">
                        </a>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Knowledge Maps: Easy to Optimize</h3>
                        <ul>
                            <li class="fragment">In a vector-based database, it is impossible to check the quality of the data segmentation,
                                as it consists exclusively of numerical values</li>
                            <li class="fragment">However, if you present a knowledge map to a domain expert who has no technical knowledge,
                                it can be understood and corrected immediately</li>
                            <li class="fragment">With this approach, improvements can be made with very little effort in
                                a short time, and the system can be adapted more closely to business needs.
                                In contrast to vector-based RAGs, which do not have the ability to "learn,"
                                this approach makes it easier to continuously adapt and improve the system</li>
                        </ul>
                    </section>
                    <section data-background-color="white">
                        <h3>MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models</h3>
                        <p style="text-align: left"><a href="https://arxiv.org/pdf/2308.09729v5.pdf">[Wen et al., 2023]</a></p>
                        <img src="images/GraphOfThoughts.png" alt="GraphOfThoughts" width="80%">
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Method</h3>
                        <p style="text-align: left">We show the framework of <strong class="important">MindMap</strong>,
                            which comprises three main parts:</p>
                        <ol>
                            <li><strong class="important">Evidence graph mining:</strong>
                                We begin by identifying the set of entities \(V_q\) from the raw input
                                and query the source KG \(G\) to build multiple evidence subgraphs \(G_q\)</li>
                            <li><strong class="important">Evidence graph aggregation:</strong>
                                Next, LLMs are prompted to comprehend and aggregate the retrieved evidence
                                subgraphs to build the reasoning graphs \(G_m\)</li>
                            <li><strong class="important">LLM reasoning on the mind map:</strong>
                                Last, we prompt LLMs to consolidate the built reasoning graph and
                                their implicit knowledge to generate the answer and build a mind map
                                explaining the reasoning process</li>
                        </ol>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">LLM agents are set to be the upcoming breakthrough</h3>
                        <p style="text-align: left"><a href="https://llmagents-learning.org/f24">LLM Agents Open Course:</a></p>
                        <ul>
                            <li>Foundation of LLMs</li>
                            <li>Reasoning</li>
                            <li>Planning, tool use</li>
                            <li>LLM agent infrastructure</li>
                            <li>Retrieval-augmented generation</li>
                            <li>Code generation, data science</li>
                            <li>Multimodal agents, robotics</li>
                            <li>Evaluation and benchmarking on agent applications</li>
                            <li>Privacy, safety, and ethics</li>
                            <li>Human-agent interaction, personalization, alignment</li>
                            <li>Multi-agent collaboration</li>
                        </ul>
                    </section>
                    <section data-background-color="white">
                          <img src="images/LLM_agents_frame.png" alt="LLM_agents_frame" width="70%">
                            <p>Source: <a href="https://yusu.substack.com/p/language-agents">
                            yusu substack</a></p>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Video on speech agents</h3>
                        <div style="text-align: center;">
                            <iframe width="80%" height="700" src="https://www.youtube.com/embed/ORDfSG4ltD4"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                            allowfullscreen></iframe>
                        </div>
                        <p>Source: <a href="https://github.com/Mozer/talk-llama-fast?tab=readme-ov-file">
                            GitHub talk Llama-fast</a></p>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">LLM-powered Autonomous Agent System</h3>
                        <div style="text-align: center;">
                            <img src="images/agent-overview.png" alt="Agent System Overview" style="width: 80%;"><br>
                            <a href="https://lilianweng.github.io/posts/2023-06-23-agent/" target="_blank">
                                https://lilianweng.github.io/posts/2023-06-23-agent/
                            </a>
                        </div>
                    </section>
                </section>
                <section>
                <section>
                    <h3 style="text-align: left">Summary</h3>
                    <ol>
                        <li class="fragment">
                            <strong>Retrieval-Augmented Generation (RAG)</strong>
                            <ul class="custom-list">
                                <li>Combines large language models (LLMs) with external document retrieval, enhancing performance</li>
                                <li class="minus">Challenges include indexing, retrieval precision,
                                    and maintaining relevance in long context windows</li>
                            </ul>
                        </li>
                        <li class="fragment">
                            <strong>Vector and Knowledge Graph-Based Retrieval</strong>
                            <ul class="custom-list">
                                <li>Vector databases like FAISS, Milvus, and ElasticSearch enable fast similarity search in high-dimensional spaces</li>
                                <li>Knowledge Graphs (KGs) can enhance retrieval by embedding structural and relational data into LLM contexts</li>
                                <li>MindMap is a recent technique using KGs for structured retrieval and reasoning within LLMs</li>
                            </ul>
                        </li>
                    </ol>
                </section>
                    <section>
                        <ol start="3">
                            <li>
                                <strong>Sparse Mixture-of-Experts (SMoE) and SSM</strong>
                                <ul class="custom-list">
                                    <li>MoEs reduce computation by selectively activating certain model parts (experts) during inference</li>
                                    <li>Jamba, a hybrid model, combines transformers with SSMs and MoE layers for long-sequence efficiency</li>
                                    <li class="minus">Complex to implement and tune but provides excellent efficiency gains
                                        in large-context applications</li>
                                </ul>
                            </li>
                            <li class="fragment">
                                <strong>LLM Agents</strong>
                                <ul class="custom-list">
                                    <li>LLM agents leverage retrieval, memory, and planning tools for automated,
                                        goal-oriented tasks</li>
                                    <li>Examples include CodeInterpreter, search tools, calculators,
                                        and agents for multimodal tasks</li>
                                    <li class="minus">Challenges: complex interactions between tools, planning inefficiencies,
                                        and agent alignment</li>
                                </ul>
                            </li>
                        </ol>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>