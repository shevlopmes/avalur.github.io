<!doctype html>
<html lang="en" xmlns="http://www.w3.org/1999/html">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>Model compression and Mixture-of-Experts</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/black.css">

        <!-- Theme used for syntax-highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
			.important {
				color: orange;
			}
			ul.custom-list {
				list-style: none;
				padding: 0;
			}
			ul.custom-list li::before {
				content: '+';
				margin-right: 8px;
                color: green;
                font-size: larger;
			}
			ul.custom-list li.minus::before {
				content: '-';
                color: red;
                font-size: larger;
			}
			ul.custom-list li.lamp::before {
				content: 'üí°';
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
                    <section>
                        <div>
                            <img src="./images/nup_logo_dark.jpeg" alt="nup_logo" />
                        </div>
                        <h2>Advanced machine learning</h2>
                        <div class="fragment" style="margin-bottom:20px;">
                                <div class="typesetting">
                                    <h3>Model compression and Mixture-of-Experts</h3>
                                    <br />
                                    Alex Avdiushenko<br />
                                    April 23, 2025
                                </div>
                        </div>
                    </section>
                    <section>
                      <h3 style="text-align: left">Lecture Plan</h3>
                      <ol>
                        <li>Quantization and Q-LoRA</li>
                        <li>Pruning</li>
                        <li>Distillation</li>
                        <li>Sparse Mixture of Experts</li>
                      </ol>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Motivation for model compression</h3>
                        <a href="https://informationisbeautiful.net/visualizations/the-rise-of-generative-ai-large-language-models-llms-like-chatgpt/">
                            <img src="images/LLM_sizes.png" alt="LLM_sizes" width="55%">
                        </a>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Motivation for model compression</h3>
                        <ul>
                            <li>LLM models are more and more massive</li>
                            <li>One week of ChatGPT usage ~= all the training cost</li>
                            <li>How can we cheaply, efficiently, and equitably deploy
                                them without sacrificing performance?</li>
                            <li class="fragment" data-fragment-index="0">Model compression
                                <ul>
                                    <li><span class="important">Quantization:</span> keep the model the same but reduce the number of bits</li>
                                    <li><span class="important">Pruning:</span> remove parts of a model while retaining performance</li>
                                    <li><span class="important">Distillation:</span> train a smaller model to imitate the bigger model</li>
                                </ul>
                            </li>
                        </ul>
                    </section>
                    <section data-background-color="white">
                        <p style="text-align: left">Why is it possible to remove part of a big model?</p>
                        <p style="text-align: left"><span class="important">In short:</span>
                            we don‚Äôt need all the parameters
                            for the inference, but we need them for training.</p>
                        <br><br>
                        <p class="fragment">We know for sure that half the stocks in our portfolio are worthless ‚Äî
                            the only problem is, we just don't know which half!</p>
                    </section>
                    <section data-background-color="white">
                          <p style="text-align: left">Post-Training Quantization</p>
                        <br>
                        <img src="images/post-train-quantization.png" alt="post-train-quantization">
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">Int8 Quantization</h3>
                        <div style="text-align: left;">
                            <ul>
                                <li>Absolute Maximum (absmax) quantization:</li>
                            </ul>
                            <div class="fragment" data-fragment-index="1">
                                $$
                                X_{i8} = \left\lfloor \frac{127 \cdot X_{f16}}{\max_{ij} (|X_{f16ij}|)} \right\rceil
                                $$
                            </div>
                            <ul class="fragment" data-fragment-index="1">
                                <li>This scales inputs to [-127, 127]</li>
                            </ul>
                            <div class="fragment" data-fragment-index="2">
                                <p>For 1d array [ 0.5, 20, -0.0001, -0.01, -0.1 ]</p>
                                <ul>
                                    <li>Maximum entry is 20</li>
                                    <li>round(127/20 * [ 0.5, 20, -0.0001, -0.01, -0.1 ]) -> [ 3, 127, 0, 0, -1 ]</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">Layer-by-Layer Quantization-Aware Distillation</h3>
                        <p style="text-align: left;"><a href="https://arxiv.org/pdf/2206.01861.pdf">[Yao et al. 2022]</a></p>
                        <div style="text-align: left;">
                            <ul>
                                <li>Initialize the quantized network with the same architecture as the original</li>
                                <li>Train each layer of the quantized network to mimic the output of its full-precision counterpart</li>
                            </ul>
                            <br><br>
                            <div class="fragment" style="text-align: center;">
                                <img src="images/quantization.png" alt="Layer-by-Layer Quantization Illustration" style="width: 80%;">
                            </div>
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">Q-LoRA</h3>
                        <p style="text-align: left;"><a href="https://arxiv.org/pdf/2305.14314.pdf">[Dettmers et al. 2023]</a></p>
                        <div style="text-align: left;">
                            <div class="fragment" data-fragment-index="0" style="text-align: center;">
                                <img src="images/QLoRA.png" alt="QLoRA Figure" style="width: 80%;">
                            </div>
                            <ul class="fragment" data-fragment-index="0">
                                <li><strong>Can train a 65B model on a 48GB GPU!</strong></li>
                            </ul>
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Pruning</h3>
                        <p style="text-align: left">Remove parameters from the model after training</p>
                        <br>
                        <div class="fragment" style="text-align: left;">
                        <h3 style="text-align: left;">Pruning vs Quantization</h3>
                            <ul>
                                <li><strong class="important">Quantization:</strong> no parameters are changed, up to $k$ bits of <em>precision</em></li>
                                <li><strong class="important">Pruning:</strong> a number of parameters are set to zero, the rest are unchanged</li>
                            </ul>
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">Magnitude Pruning</h3>
                        <p style="text-align: left;">
                            <a href="https://arxiv.org/abs/1606.09274">[See et al. 2016]</a>,
                            <a href="https://arxiv.org/abs/1510.00149">[Han et al. 2015]</a>
                        </p>
                        <div style="text-align: left;">
                            <ul>
                                <li>Zero out the X% of parameters with the least magnitude</li>
                                <li>A type of <em>unstructured pruning</em></li>
                            </ul>
                        </div>
                        <div class="fragment" style="text-align: center;">
                            <img src="images/BLEU_prune.png" alt="Magnitude Pruning BLEU Score Graph" style="width: 40%;">
                            <p>Graph showing the BLEU score versus percentage of parameters pruned</p>
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">Structured Pruning</h3>
                        <p style="text-align: left;"><a href="https://arxiv.org/abs/2204.00408">[Xia et al. 2022]</a></p>
                        <div style="text-align: left;">
                            <ul>
                                <li>Remove entire components</li>
                                <li>The remaining components aren‚Äôt pruned</li>
                            </ul>
                        </div>
                        <div class="fragment" style="text-align: center;">
                            <img src="images/struct_prune.png" alt="Structured Pruning Diagram" style="width: 70%;">
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">Pruning with only Forward Passes</h3>
                        <p style="text-align: left;"><a href="https://arxiv.org/pdf/2402.05406">
                            [Dery et al. 2024]</a></p>
                        <div style="text-align: left;">
                            <ul>
                                <li>Structured pruning of big models requires a lot of memory</li>
                                <li>Can we avoid using gradients?</li>
                                <li class="fragment" data-fragment-index="0"><span class="important">Idea:</span></li>
                            </ul>
                            <div class="fragment" style="margin-left: 20px;" data-fragment-index="0">
                                <ol>
                                    <li>Measure the performance of a model with different modules masked</li>
                                    <li>Learn the impact of each module mask via regression</li>
                                </ol>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left">Distillation</h3>
                        <p style="text-align: left">Train one model (the ‚Äústudent‚Äù) to replicate the behavior of another model (the ‚Äúteacher‚Äù)</p>
                        <br>
                        <p class="fragment" data-fragment-index="0" style="text-align: left;">Distillation vs Quantization vs Pruning</p>
                        <div class="fragment" data-fragment-index="0" style="text-align: left;">
                            <ul>
                                <li><strong class="important">Quantization:</strong> no parameters are changed,
                                    up to <em>k</em> bits of <em>precision</em></li>
                                <li><strong class="important">Pruning:</strong> a number of parameters are set to zero,
                                    the rest are unchanged</li>
                                <li><strong class="important">Distillation:</strong> ~all parameters are changed</li>
                            </ul>
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">DistilBERT</h3>
                        <p style="text-align: left;"><a href="https://arxiv.org/abs/1910.01108">[Sanh et al. 2019]</a></p>
                        <div style="text-align: left;">
                            <ul>
                                <li><a href="https://arunm8489.medium.com/understanding-distil-bert-in-depth-5f2ca92cf1ed">https://arunm8489.medium.com/understanding-distil-bert-in-depth-5f2ca92cf1ed</a></li>
                                <li>Uses half the layers and 60% of total parameters</li>
                                <li class="fragment" data-fragment-index="0">Tricks:
                                    <ul>
                                        <li>Initialize DistilBERT with alternating layers from BERT</li>
                                        <li>Use both supervised and distillation-based losses</li>
                                        <ul>
                                            <li>Supervised loss doesn‚Äôt help much</li>
                                        </ul>
                                        <li>Add cosine similarity of hidden state vectors between teacher and student</li>
                                    </ul>
                                </li>
                            </ul>
                        </div>
                        <div class="fragment" data-fragment-index="1" style="text-align: center;">
                            <img src="images/DistilBERT.png" alt="DistilBERT Performance Table" style="width: 70%;">
                            <p>Performance comparison between models</p>
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">A Toolkit for Synthetic Data Generation</h3>
                        <p style="text-align: left;"><a href="https://arxiv.org/pdf/2402.10379.pdf">[Patel et al. 2024]</a></p>
                        <div style="text-align: left;">
                            <ul>
                                <li>"Hard target distillation"</li>
                            </ul>
                        </div>
                        <div class="fragment" style="text-align: center;">
                            <img src="images/DataDreamer.png" alt="Toolkit for Synthetic Data Generation Table" style="width: 80%;">
                        </div>
                    </section>
                </section>
                <section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">Mixture of Experts: Sparse Computation</h3>
                        <div style="text-align: left;">
                            <ul>
                                <li>What happens when a scalar-tensor multiplication is zero?</li>
                                <div class="fragment" data-fragment-index="0" style="text-align: center;">
                                    <p>$0 \cdot [a, b, c] = [0, 0, 0]$</p>
                                </div>
                                <li class="fragment" data-fragment-index="0">
                                    The result is guaranteed to be zero! No computation needed</li>
                                <li class="fragment" data-fragment-index="1">
                                    This can happen in many parts of a model:
                                    <ul>
                                        <li>Single rows in a matrix multiply  ‚Üí  <span style="color: blue;">optimized by GPU</span></li>
                                        <li>Larger tensors  ‚Üí  <span style="color: orange;">sparse MoE models</span></li>
                                        <li>Whole models in an ensemble  ‚Üí  <span style="color: blue;">just don‚Äôt use that model</span></li>
                                    </ul>
                                </li>
                            </ul>
                        </div>
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">Sparsely Gated Mixture of Experts Layer</h3>
                        <p style="text-align: left;"><a href="https://arxiv.org/abs/1701.06538">[Shazeer et al. 2017]</a></p>
                        <div style="text-align: left;">
                            <ul>
                                <li>Select a subset of FFNs to actually execute</li>
                            </ul>
                        </div>
                        <div class="fragment" style="text-align: center;">
                            <img src="images/MoE_layer.png" alt="MoE Layer Diagram" style="width: 60%;">
                        </div>
                    </section>
                    <section data-background-color="white">
                        <p>
                            $$
                            G(x) = \text{softmax}(\text{keep\_top\_k}(f_\text{gating}(x), k))
                            $$
                        </p>
                        <p>
                            $$
                            \text{keep\_top\_k}(v, k)_i = \begin{cases}
                            v_i & \text{if } v_i \text{ is in the top k elements of } v \\
                            -\infty & \text{otherwise}
                            \end{cases}
                            $$
                        </p>
                    </section>
                    <section data-background-color="white">
                        <img src="images/Mixtral_res.png" alt="Mixtral_res.png" width="80%">
                    </section>
                    <section data-background-color="white">
                        <h3 style="text-align: left;">Understanding Mixtral-8x7b</h3>
                        <div style="text-align: left;">
                            <p>The SMoE (Sparse-Mixture-of-Experts) MLP is Mixtral's distinct feature.
                                It is the reason for its exceptional performance at low compute cost.</p>
                            <ul class="fragment">
                                <li>SMoEs trade high parameter count for computational efficiency
                                    by using multiple layers (experts) but only executing a few per input</li>
                                <li>The gating mechanism uses a learned linear layer to score each expert,
                                    and the top $k$ experts are selected for computation</li>
                                <li>The weighted summation only needs to occur over the chosen top experts, saving computation</li>
                            </ul>
                        </div>
                        <div class="fragment" style="text-align: center;">
                            <a href="https://huggingface.co/blog/vtabbott/mixtral">
                                https://huggingface.co/blog/vtabbott/mixtral
                            </a>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                      <h3 style="text-align: left">Summary</h3>
                      <ol>
                        <li class="fragment">
                          <strong>Quantization and Q-LoRA</strong>
                          <ul class="custom-list">
                            <li>Quantization reduces the precision of model parameters (e.g., Int8) to save memory and computational resources</li>
                            <li>Q-LoRA allows fine-tuning large models (e.g., 65B) on resource-constrained hardware by using 4-bit quantization</li>
                            <li class="minus">May introduce small accuracy loss if precision is too low</li>
                          </ul>
                        </li>
                        <li class="fragment">
                          <strong>Pruning</strong>
                          <ul class="custom-list">
                            <li>Pruning removes less significant parameters, reducing model size and computation</li>
                            <li>Magnitude pruning zeros out the lowest magnitude parameters, while structured pruning removes entire components</li>
                            <li class="minus">Structured pruning requires careful tuning to avoid significant performance drops</li>
                          </ul>
                        </li>
                      </ol>
                    </section>
                    <section>
                        <ol start="3">
                            <li>
                              <strong>Distillation</strong>
                              <ul class="custom-list">
                                <li>Distillation trains a smaller model (student) to imitate a larger model (teacher), retaining most of its performance</li>
                                <li>DistilBERT is an example, achieving comparable results to BERT with fewer parameters</li>
                                <li class="minus">Involves some performance trade-offs depending on task complexity</li>
                              </ul>
                            </li>
                            <li class="fragment">
                              <strong>Sparse Mixture of Experts (MoE)</strong>
                              <ul class="custom-list">
                                <li>Sparse MoE only activates a subset of experts (layers) for each input, drastically reducing computation</li>
                                <li>Mixtral-8x7b uses SMoE for efficient computation with high performance</li>
                                <li class="minus">Requires a gating mechanism to determine which experts to use for each input</li>
                              </ul>
                            </li>
                            <li class="fragment">
                                What else to look at?
                                <a href="https://www.youtube.com/watch?v=fk2r8y5TfNY">
                                    Scientific knowledge distillation from deep learning models
                                </a>
                            </li>
                        </ol>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>