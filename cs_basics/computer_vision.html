<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>Computer vision intro</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/black.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">
		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
		</style>
</head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
				<section>
					<div>
						<img src="images/nup_logo_dark.jpeg" alt="nup_logo" />
					</div>
					<h2>CS Basics with Python</h2>
					<h3>Computer vision intro</h3>
					<br />
					Alex Avdiushenko <br />
					November 29, 2023
				</section>
				<section>
					<section>
						<h2>What is computer vision?</h2>
						<div>
							<img src="images/cv_search.png" alt="cv_search" />
						</div>
					</section>
					<section>
						<h2>What is vision?</h2>
						<ul>
							<li>Eyes are the human organ that provides 80 percent of information about
								the surrounding world to a person. (Though, it depends on how you calculate it)</li>
							<li>Vision is the extraction of information from the visual signal hitting the retina</li>
						</ul>
						<div>
							<img src="images/Illustration_of_a_detailed_human_eye_anatomy.png" alt="vision" width="40%"/>
						</div>
					</section>
					<section>
						<h2>Computer vision</h2>
						<ul>
							<li>The same concept, only instead of the brain it's a computer,
								and instead of an eye it's a camera</li>
						</ul>
						<div>
							<img src="images/cv_illustration.png" alt="cv_illustration" width="40%"/>
						</div>
					</section>
					<section>
						<h2>General Challenge in Computer Vision</h2>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
							<p>To learn how to respond to the same questions that a person can answer by looking at a photo or video.
								This is a kind of Turing test for a computer vision system.</p>
							</div>
						</div>
					</section>
					<section>
						<h2 style="text-align: left">Our Dreams</h2>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
								<div class="row">
								  <div class="column">
									<img src="images/cv_today.png" alt="cv_today" />
								  </div>
								  <div class="column">
									<ul>
										<li>For the computer to "understand" the semantics of the scene in the image</li>
										<li>To automatically recognize what and where is depicted</li>
										<li>To categorize and identify objects, determine their properties and relationships</li>
									</ul>
								  </div>
								</div>
							</div>
						</div>
					</section>
					<section>
						<h2>Existing Obstacles: Object</h2>
						<img src="images/obstacles.png" alt="obstacles" width="80%"/>
					</section>
					<section>
						<h2>How We See an Object</h2>
						<img src="images/lamps.png" alt="lamps" width="80%"/>
					</section>
					<section>
						<h2>Semantic gap: from low-level features to the objects</h2>
						<img src="images/low_to_high.png" alt="low_to_high" width="70%"/>
					</section>
				</section>
				<section>
					<section>
						<h2>A few facts about our visual perception:</h2>
							<div class="fragment" style="margin-bottom:20px;">
								<div class="typesetting">
								<ol>
									<li>Our brain often "completes" the picture and adds semantics (We can all recognize "something" or "someone" in the outline of a cloud)</li>
									<li>The visual system is self-learning
										<ul>
										   <li>It's difficult for Europeans to distinguish Asian faces, and vice versa</li>
										   <li>We look for familiar patterns in images</li>
										   <li>Always we are trying to predict (with strong internal prior) the whole picture</li>
										</ul>
									</li>
								</ol>
								</div>
							</div>
					</section>
					<section>
						<img src="images/optical-b1.gif" alt="optical" width="50%"/>
					</section>
					<section>
						<img src="images/optical-b-explained.gif" alt="optical-b-explained" width="100%"/>
					</section>
					<section>
						<h2>Brightness Adaptation and Contrast Sensitivity</h2>
						<p>The visual system can adapt to a brightness range of
							around $10^{10}$. The subjective brightness
							is a logarithmic function of the physical brightness.</p>
					</section>
					<section>
						<h2 style="text-align: left">Photopic and Scotopic vision</h2>
							<div class="fragment" style="margin-bottom:20px;">
								<div class="typesetting">
								Are two different types of vision that humans possess,
								and they function under different light conditions.
								<ul>
									<li>Photopic Vision: This type of vision dominates in well-lit conditions,
										such as in daylight or under bright artificial lighting.
										It involves the use of <span style="color: orange">cones</span>,
										the photoreceptor cells responsible for color perception.
										It permits color and high acuity vision to see fine details.</li>
									<li>Scotopic vision takes over in low light or night-time conditions.
										It involves the use of <span style="color: orange">rods</span>,
										that are much more sensitive to light than cones but do not provide
										color information and doesn't allow for the resolution of fine details.</li>
								</ul>
								</div>
							</div>
					</section>
				</section>
				<section>
					<section>
						<h2 style="text-align: left">Whirlwind, MIT 1951</h2>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
								<div class="row">
								  <div class="column">
									<ul>
										<li>The first computer to display text and graphics in real time on a monitor</li>
										<li>Displayed the map with dots, the airplane with an icon</li>
										<li>Used a "light pen" for interaction with the screen (requesting information about an object)</li>
									</ul>
								  </div>
								  <div class="column">
									<img src="images/mit_1951.png" alt="mit_1951" width="70%" />
								  </div>
								</div>
							</div>
						</div>
					</section>
					<section>
						<h2 style="text-align: left">The Birth of Computer Vision (1960)</h2>
						<p>Determining the mutual arrangement of simple geometric figures</p>
						<a href="https://www.di.ens.fr/willow/teaching/recvis09/slides/lecture1.pdf">
							<img src="images/cv_origin.png" alt="cv_origin" width="60%" />
						</a>
					</section>
					<section>
						<h2>Viola-Jones face detector, 2001</h2>
						<img src="images/lena.png" alt="lena" width="50%" />
					</section>
					<section>
						<h2>Deep learning, 2010+</h2>
						<img src="images/dogsplayingpoker_3370414k.jpg" alt="dogsplayingpoker" width="70%" />
					</section>
				</section>
				<section>
					<section>
						<h2>What is color?</h2>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
								<p>Color is a psychological property of our vision that arises when observing objects and light,
									not the physical properties of objects and light <br>
									(S. Palmer, Vision Science: Photons to Phenomenology).</p>
							</div>
						</div>
					</section>
					<section data-background-color="#ffffff">
						<h2>But what about this spectrum and all that?</h2>
						<img src="images/what_about_meme.png" alt="what_about_meme">
					</section>
					<section data-background-color="#ffffff">
						<img src="images/spectrum.png" alt="spectrum">
					</section>
					<section>
						<h2>Visible Light</h2>
						<ul>
							<li>Electromagnetic radiation in the range [380nm,780nm]</li>
							<li>Visible light falls within the main "optical window" of the Earth's atmosphere (~46% energy)</li>
						</ul>
					</section>
					<section>
						<h2>Light physics</h2>
						<img src="images/light_physics.png" alt="light_physics" width="60%">
					</section>
					<section data-background-color="#ffffff">
						<h2>Reflected light</h2>
						<img src="images/reflected_light.png" alt="reflected_light" width="80%">
					</section>
					<section data-background-color="#ffffff">
						<h2>Examples of Reflection Spectra</h2>
						<img src="images/spectra_examples.png" alt="spectra_examples" width="80%">
					</section>
					<section>
						<h2 style="color: orange">Rods and Cones</h2>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
								<ul>
									<li>Rods and cones are spectrum filters.
							The spectrum is multiplied by the response curve,
							integration is performed across all wavelengths —
							each of three types of cone gives 1 number:
								S, M, L. </li>
									<li>How can we describe the entire spectrum with
							just 3 numbers?</li>
									<li>
								The fact is we cannot!
							A significant amount of information is lost.
							Two different spectra can be indistinguishable.
									</li>
								</ul>
							</div>
						</div>
					</section>
					<section data-background-color="#ffffff">
						<img src="images/rods_and_cones_sensitivity.png" width="60%">
					</section>
					<section data-background-color="#ffffff">
						<h3>Metamers — different spectra of the same color</h3>
						<img src="images/metamers.png" width="70%">
					</section>
				</section>
				<section>
					<section data-background-color="#ffffff">
						<h2>RGB model</h2>
						<a href="http://sv-journal.org/2015-4/03/en/index.php?lang=en">
							<img src="images/CIE_RGB_curves.png" width="60%">
						</a>
					</section>
					<section>
						<h2>CIE XYZ</h2>
						<ul>
							<li>One of the first mathematically defined color spaces
								that could represent all colors visible to the average human</li>
							<li>Is based on the idea that every color perceived by the human eye
								can be represented as a combination of three primary colors (X, Y, Z)</li>
							<li><b>Color Gamut and Color Spaces:</b>
								All colors that can be represented by a specific set of primaries forms a color gamut.
								The CIE XYZ color space serves as a reference space to compare
								other color spaces or gamuts.</li>
						</ul>
					</section>
					<section data-background-color="#ffffff">
						<img src="images/gamuts.png" width="80%">
					</section>
					<section>
						<h2>Digital Image Formats</h2>
						<p style="text-align: left">Current typical display adapters use up to <b>24-bits of information for each pixel</b>:
						8-bit per component multiplied by three components (RGB) with values of 0–255.</p>

						<p style="text-align: left">With this system, $16\ 777\ 216 ( = 256^3 = 2^{24}$) discrete combinations
							of R, G, and B values are allowed, providing millions of different
							(though not necessarily distinguishable) hue, saturation and lightness shades.</p>
					</section>
					<section>
						<h2 style="text-align: left"><a href="https://en.wikipedia.org/wiki/GIF">GIF</a></h2>
						<p style="text-align: left">Graphics Interchange Format — bitmap image format.
						GIF images are compressed using the <a href="https://en.wikipedia.org/wiki/Lempel–Ziv–Welch">Lempel–Ziv–Welch</a> (LZW)
							lossless data compression technique to reduce the file size without degrading the visual quality.</p>
						<h2 style="text-align: left"><a href="https://en.wikipedia.org/wiki/Portable_Network_Graphics">PNG</a></h2>
						<p style="text-align: left">Portable Network Graphics — was developed as an improved, <b>non-patented</b>
							replacement for GIF. Unofficially, the initials PNG stood for the recursive acronym "PNG's not GIF", also lossless.</p>
					</section>
					<section>
						<h2 style="text-align: left"><a href="https://en.wikipedia.org/wiki/JPEG">JPEG</a></h2>
						<ul style="text-align: left">
							<li>Joint Photographic Experts Group — is a commonly used method of
							lossy compression for digital images, particularly produced by digital photography.</li>
							<li>The degree of compression can be adjusted, allowing a selectable tradeoff
								between storage size and image quality.</li>
							<li>JPEG typically achieves 10:1 compression with little perceptible loss in image quality.
							It uses discrete cosine transform and quantization.</li>
						</ul>
					</section>
					<section>
						<h2 style="text-align: left"><a href="https://en.wikipedia.org/wiki/WebP">WebP</a></h2>
						<ul style="text-align: left">
							<li>WebP is a raster graphics file format developed by Google
							intended as a replacement for JPEG, PNG, and GIF file formats =)</li>
							<li>It supports both lossy and lossless compression, as well as animation and alpha transparency.</li>
							<li>Google announced the WebP format in September 2010, and released
								the first stable version of its supporting library in April 2018.</ul>
					</section>
				</section>
				<section>
					<h2 style="text-align: left"><a href="https://colab.research.google.com/github/avalur/avalur.github.io/blob/master/cs_basics/python_img_practice.ipynb" class="mt-auto btn btn-primary">
						Python image practice</a></h2>
				</section>
				<section>
					<h2>Dream big, work hard.</h2>
				</section>
			</div>
		</div>

		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});

		</script>
    </body>
</html>
