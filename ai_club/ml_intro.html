<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>Machine Learning Intro</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/sky.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
		<script defer src="https://pyscript.net/latest/pyscript.js"></script>
		<py-config>
			terminal = false
            packages = ["numpy", "matplotlib", "scikit-learn"]
		</py-config>
		<style>
			.column {
			  float: left;
			  width: 48%;
			  padding: 2px;
			}
			/* Clear floats after image containers */
			.row::after {
			  content: "";
			  clear: both;
			  display: table;
			}
			.important {
				color: orange;
			}
		</style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
                    <section>
                        <div class="column">
                            <img src="images/jetbrains_logo_icon_145150.webp" alt="jetbrains_logo" />
                        </div>
                        <div class="column">
                            <h2>Youth AI club</h2>
                            <div class="fragment" style="margin-bottom:20px;">
                                    <div class="typesetting">
                                        <h3>Lecture 3. Introduction to Machine Learning</h3>
                                        <br />
                                        Alex Avdiushenko <br />
                                        October 16, 2024
                                    </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">In the last episode</h3>
                        <div class="fragment" style="text-align: left">
                            <div class="typesetting">
                                <ul>
                                    <li><span class="important">Numpy</span> arrays:
                                        creation, slices, and indexation</li>
                                    <li>Broadcasting in Numpy</li>
                                    <li>Matrix multiplication</li>
                                    <li><span class="important">Pandas</span> is almost "Numpy with titles and SQL"</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <h3>What is ML?</h3>
    					<div class="fragment" style="margin-bottom:20px;">
                            <img src="../ml_with_python/images/ml_def_from_chatGPT.png" alt="ml_def_from_chatGPT">
                        </div>
                    </section>
                    <section>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">ML is built using the tools of mathematical statistics,
                                numerical methods, mathematical analysis, optimization methods, probability theory,
                                    and various techniques for working with data in digital form.</p>
                                <br />
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p class="r-frame important" style="text-align: left;padding-left: 10px">Before ML, we had to create math models:</p>
                                \[\begin{cases}
                                    \frac{\partial \rho}{\partial t} + \frac{\partial(\rho u_{i})}{\partial x_{i}} = 0 \\ \\
                                    \frac{\partial (\rho u_{i})}{\partial t} + \frac{\partial[\rho u_{i}u_{j}]}{\partial x_{j}} = -\frac{\partial p}{\partial x_{i}} + \frac{\partial \tau_{ij}}{\partial x_{j}} + \rho f_{i}
                                  \end{cases}
                                \]
                            </div>
                        </div>
                    </section>
                    <section>
                        <iframe width="1400" height="650" src="https://www.youtube.com/embed/5028bTvDwO8"></iframe>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p>In machine learning, there is no pre-set model with equations...</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Types of Machine Learning</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                  <li class="fragment"><strong class="important">Supervised Learning:</strong> The algorithm is provided with
                                      <span class="important">labeled</span> training data, and the goal is to learn a general rule
                                      that maps inputs to outputs</li>
                                  <li class="fragment"><strong class="important">Unsupervised Learning:</strong> The algorithm is provided with
                                      <span class="important">unlabeled</span> data and must find the underlying structure in the data,
                                      like clustering or dimensionality reduction</li>
                                  <li class="fragment"><strong class="important">Reinforcement Learning:</strong> The algorithm
                                      <span class="important">learns by interacting</span> with an environment
                                      and receiving feedback in the form of rewards or penalties</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Main notations</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <p style="text-align: left">$X$ — set of objects <br>
                            $Y$ — set of answers</p>

                            <p>$y: X \to Y$ — unknown dependence (target function)</p>

                            <p>The task is to find, based on the training sample $\{x_1,\dots,x_\ell\} = X_\ell \subset X$
                                with known answers $y_i=y(x_i)$, <br>
                                an algorithm ${\color{orange}a}: X \to Y$, <br>
                                which is a decision function that approximates $y$ over the entire set $X$</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <p style="text-align: left">The whole ML course is about this:</p>
                        <ul>
                          <li>how objects $x_i$ are defined and what answers $y_i$ can be</li>
                          <li>in what sense ${\color{orange}a}$ approximates $y$</li>
                          <li>how to construct the function ${\color{orange}a}$</li>
                        </ul>
                    </section>
                    <section>
                            <p>$f_j: X \to D_j$ </p>

                            <p style="text-align: left">The vector $(f_1(x), \dots, f_n(x))$ is so called "feature description" of the object $x$</p>

                            <p style="text-align: left">Types of features:</p>
                            <ul>
                              <li>$D_j = \{0, 1\}$ — binary</li>
                              <li>$\#|D_j| < \infty $ — categorical</li>
                              <li>$\#|D_j| < \infty, D_j$ ordered — ordinal</li>
                              <li>$D_j = \mathbb{R}$ — real (quantitative)</li>
                            </ul>

                            <div class="fragment" style="margin-top:20px;">
                                <div class="r-frame" style="color: orange">
                                <b>Question 1:</b> How to convert all features to binary?
                            </div>
                        </div>
                    </section>
                    <section>
                        <p style="text-align: left">Data matrix (objects and features as rows and columns):</p>

                        <p><br />
                            $F = ||f_j(x_i)||_{\ell\times n} = \left[ {\begin{array}{ccc}
                           f_1(x_1) & \dots & f_n(x_1) \\
                             \vdots  & \ddots &   \vdots  \\
                           f_1(x_\ell) & \dots & f_n(x_\ell)
                          \end{array} } \right]$
                        </p><br />

                    </section>
                    <section>
                        <h3>Types of tasks</h3>

                        <h4 style="text-align: left">Classification</h4>
                        <ul>
                          <li>$Y = \{-1, +1\}$ — binary classification</li>
                          <li>$Y = \{1, \dots, M\}$ — multiclass classification</li>
                          <li>$Y = \{0, 1\}^M$ — multiclass with overlapping classes</li>
                        </ul>

                        <h4 style="text-align: left">Regression</h4>
                          <p>$Y = \mathbb{R}\ $ or $\ Y = \mathbb{R}^m$</p>

                        <h4 style="text-align: left">Ranking</h4>
                          $Y$ — finite ordered set
                    </section>
                    <section>
                        <h3>Predictive Model</h3>

                        <p style="text-align: left">A model (predictive model) — a parametric family of functions</p>

                        <p>$A = \{g(x, \theta) | \theta \in \Theta\}$,</p>

                        <p style="text-align: left">where $g: X \times \Theta \to Y$ — a fixed function, $\Theta$ — a set of allowable
                            values of parameter $\theta$</p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <h4 style="text-align: left">Example</h4>

                            <p>Linear model with vector of parameters
                                $\theta = (\theta_1, \dots, \theta_n), \Theta = \mathbb{R}^n$:</p>

                            <p>$g(x, \theta) = \sum\limits_{j=1}^n \theta_jf_j(x)$ — for regression and ranking,
                                $Y = \mathbb{R}$</p>

                            <p>$g(x, \theta) = \mathrm{sign}\left(\sum\limits_{j=1}^n \theta_jf_j(x)\right)$ — for classification,
                                $Y = \{-1, +1\}$</p>
                            </div>
                        </div>
                    </section>
                </section>
                <section id="lin_reg_in_Python">
                    <section data-background-color="#fdf6e3">
                        <h3>Example: Regression Task, Synthetic Data</h3>

                        <p style="text-align: left">$X = Y = \mathbb{R}$, $\ell = 50$ objects</p>

                        <p>$n = 3$ features: $\{1, x, x^2\}$ or $\{1, x, \sin x\}$</p>
                        <py-repl>
                            import matplotlib.pyplot as plt
                            import numpy as np
                            np.random.seed(0)

                            l = 50
                            x = np.linspace(0, 30, num=l)
                            Y = x + 4*np.sin(x) + 3*np.random.randn(l)

                            X1 = np.vstack([np.ones_like(x), x, x**2]).T
                            X2 = np.vstack([np.ones_like(x), x, np.sin(x)]).T
                            print(X1.shape, X2.shape)
                        </py-repl>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <py-repl>
                            fig, ax = plt.subplots()
                            ax.set_xlabel('x'), ax.set_ylabel('Y')
                            ax.plot(x, Y, '.', label='train points')

                            x_plot = np.linspace(0, 30, num=1000)
                            ax.plot(x_plot, x_plot + 4*np.sin(x_plot), label='real')
                            plt.legend(loc='best')
                            fig # plt.show()
                        </py-repl>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <py-repl>
                            from sklearn.linear_model import LinearRegression

                            reg1 = LinearRegression(fit_intercept=False)
                            reg1.fit(X1, Y)

                            reg2 = LinearRegression(fit_intercept=False)
                            reg2.fit(X2, Y)

                            print("Models have been trained.")
                        </py-repl>
                        <div class="fragment important" style="margin-top:20px;">
                            <div class="typesetting">
                                <div class="r-frame">
                                    <b>Question 2:</b> What does an <i>intercept</i> mean?
                                </div>
                            </div>
                        </div>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <py-repl>
                            fig, ax = plt.subplots()
                            ax.set_xlabel('x'), ax.set_ylabel('Y')
                            ax.plot(x, Y, '.', label='train points')

                            x_plot = np.linspace(0, 30, num=1000)
                            X1 = np.vstack([np.ones_like(x_plot), x_plot, x_plot**2]).T
                            X2 = np.vstack([np.ones_like(x_plot), x_plot, np.sin(x_plot)]).T

                            print("Data for plots have been created.")
                        </py-repl>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <py-repl>
                            ax.plot(x_plot, reg1.predict(X1), label='{1, x, x^2}', linestyle='dashed')
                            ax.plot(x_plot, reg2.predict(X2), label='{1, x, sin(x)}')
                            ax.plot(x_plot, x_plot + 4*np.sin(x_plot), label='real')
                            plt.legend(loc='best')
                            fig
                        </py-repl>
                    </section>
                    <section>
                        <h3>Let's have a break!</h3>
                    </section>
                </section>
                <section>
                    <section>
                        <h3 style="text-align: left">Training Stage</h3>
                        <p style="text-align: left">The method $\mu$ constructs an algorithm $a = \mu(X_\ell, Y_\ell)$ from the sample
                            $(X_\ell, Y_\ell) = (x_i, y_i)_{i=1}^\ell$</p>

                        <p>$\boxed{
                        \left[ {\begin{array}{ccc}
                           f_1(x_1) & \dots & f_n(x_1) \\
                             \dots  & \dots &   \dots  \\
                           f_1(x_\ell) & \dots & f_n(x_\ell)
                          \end{array} } \right]
                        \xrightarrow{y}
                        \left[ {\begin{array}{c}
                        y_1 \\ \dots \\ y_\ell \end{array} }\right]
                        \thinspace}
                        \xrightarrow{\mu} a$</p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <h3 style="text-align: left">Test Stage</h3>
                            <div class="typesetting">
                                <p>The algorithm $a$ produces answers $a(x_i^\prime)$ for new objects $x_i^\prime$</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Quality Functionals</h3>

                        <p>$\mathcal{L}(a, x)$ — loss function.
                            The error magnitude of the algorithm $a \in A$ on the object $x \in X$</p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <h4 style="text-align: left">Loss Functions for Classification Tasks:</h4>
                            <ul>
                              <li>$\mathcal{L}(a, x) = [a(x)\neq y(x)]$ — indicator of error</li>
                              <li>Cross-entropy is the distance between two vectors of probabilities
                              (will discuss next lecture)</li>
                            </ul>
                        </div>

                        <div class="fragment" style="margin-bottom:20px;">
                            <h4 style="text-align: left">Loss Functions for Regression Tasks:</h4>
                            <ul>
                              <li>$\mathcal{L}(a, x) = |a(x) - y(x)|$ — absolute error value</li>
                              <li>$\mathcal{L}(a, x) = (a(x) - y(x))^2$ — quadratic error</li>
                            </ul>
                        </div>

                        <div class="fragment" style="margin-bottom:20px;">
                            <p style="text-align: left"><em>Empirical risk</em> —
                                functional quality of the algorithm $a$ on $X^\ell$:</p>

                            <p>$Q(a, X^\ell) = \frac{1}{\ell} \sum\limits_{i=1}^\ell \mathcal{L}(a, x_i)$</p>
                        </div>
                    </section>
                    <section>
                        <h3>Reducing the Learning Task to an Optimization Task</h3>

                        <p style="text-align: left">Method of minimizing empirical risk</p>

                        <p>$\mu(X^\ell) = \arg\min\limits_{a \in A} Q(a, X^\ell)$</p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <h4 style="text-align: left">Example: least squares method</h4>
                            <p style="text-align: left">$Y = \mathbb{R}, \mathcal{L}$ is quadratic:</p>
                            $$\mu(X^\ell) = \arg\min\limits_{\theta} \sum\limits_{i=1}^{\ell} (g(x_i, \theta) - y_i)^2$$
                        </div>
                    </section>
                    <section>
                        <h4 style="text-align: left">Generalization Ability Problem</h4>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <ul>
                              <li>Will we find the "law of nature" or will we overfit, i.e.,
                                  adjust the function $g(x_i, \theta)$ to the given points?</li>
                              <li>Will $a = \mu(X^\ell)$ approximate the function $y$ over the entire $X$?</li>
                              <li>Will $Q(a, X^k)$ be small enough on new data — the test set
                                  $X^k = (x_i^\prime, y_i^\prime)_{i=1}^k$, $y_i^\prime = y(x_i)$?</li>
                            </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Simple Example of Overfitting</h3>
                        <p>Dependency $y(x) = \frac{1}{1+25x^2}$ on the interval $x \in \left[-2, 2\right]$</p>

                        <p>Feature description $x \to (1, x, x^2, \dots, x^n)$</p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <h4 style="text-align: left">Polynomial regression model:</h4>

                            <p>$a(x, \theta) = \theta_0 + \theta_1 x + \dots + \theta_n x^n$ — a polynomial of degree $n$</p>
                        </div>

                        <div class="fragment" style="margin-bottom:20px;">
                            <h4 style="text-align: left">Training using the least squares method:</h4>

                            <p>$Q(\theta, X^\ell) = \sum\limits_{i=1}^\ell (\theta_0 + \theta_1 x_i + \dots + \theta_n x_i^n - y_i)^2 \to \min\limits_{\theta_0,\dots,\theta_n}$</p>

                            <p style="text-align: left">Training sample: $X^\ell = \{x_i = 4\frac{i-1}{\ell-1} - 2 | i = 1, \dots, \ell \}$</p>

                            <p style="text-align: left">Test sample: $X^k = \{x_i = 4\frac{i-0.5}{\ell-1} - 2 | i = 1, \dots, \ell-1 \}$</p>
                        </div>
                        <div class="fragment important" style="margin-top:20px;">
                            <p class="r-frame">What happens to $Q(\theta, X^\ell)$ and $Q(\theta, X^k)$ as $n$ increases?</p>
                        </div>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <py-repl>
                            import numpy as np

                            np.random.seed(0)
                            l = 50

                            x = np.linspace(-2, 2, num=l)
                            Y = 1 / (1 + 25*x**2)
                            print(x.shape, Y.shape)
                        </py-repl>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <py-repl>
                            import matplotlib.pyplot as plt

                            fig, ax = plt.subplots()
                            ax.set_xlabel('x'), ax.set_ylabel('Y')
                            ax.plot(x, Y, '.', label='train points')
                            x_plot = np.linspace(-2, 2, num=1000)
                            y_plot = 1 / (1 + 25*x_plot**2)
                            ax.plot(x_plot, y_plot, label='real')
                            plt.legend(loc='best'); fig
                        </py-repl>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <py-repl>
                            from sklearn.linear_model import LinearRegression

                            def X(x, n):
                                res = [np.ones_like(x)]
                                for i in range(1, n):
                                    res.append(x**i)
                                return np.vstack(res).T

                            print("X has been compiled.")
                        </py-repl>
                    </section>
                    <section data-background-color="#fdf6e3">
                        <py-repl>
                            x_l = np.array([4*(i - 1)/(l - 1) - 2 for i in range(1, l + 1)])
                            x_k = np.array([4*(i - 0.5)/(l - 1) - 2 for i in range(1, l)])
                            lin_reg = LinearRegression(fit_intercept=False)

                            train_score, test_score = [], []
                            ns = range(2, 40)
                            for n in ns:
                                X_l = X(x_l, n)
                                Y_l = 1 / (1 + 25*x_l**2)
                                X_k = X(x_k, n)
                                Y_k = 1 / (1 + 25*x_k**2)
                                lin_reg.fit(X_l, Y_l)
                                train_score.append(np.mean((lin_reg.predict(X_l) - Y_l)**2))
                                test_score.append(np.mean((lin_reg.predict(X_k) - Y_k)**2))
                                print(n,end=', ')
                            print()
                        </py-repl>
                    </section>
                    <section data-background-color="#fdf6e3">
<!--language=Python-->
                        <py-repl>
                            import matplotlib.pyplot as plt_
                            fig_, ax_ = plt_.subplots()
                            ax_.set_xlabel('n'), ax_.set_ylabel('log Q')
                            ax_.set_xlim(0, len(ns) + 2)

                            ax_.plot(np.log(train_score), 'bo-', label='train score')
                            ax_.plot(np.log(test_score), 'go--', label='test score')
                            plt_.legend(loc='best')
                            # plt_.savefig('overfitting.png')
                            fig_
                        </py-repl>
                    </section>
                    <section data-background-color="#ffffff">
                        <img src="../ml_with_python/images/overfitting.png" width="50%">
                        <div class="fragment important" style="margin-top:20px;">
                            <div class="r-frame">
                            <b>Question 3:</b> Why we have such plots? How will the divergence point $n$ change
                                depending on the size of the training sample $\ell$ and why?
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Overfitting — one of the main problems in machine learning</h3>

                        <p>When <strong>test_score >> train_score</strong></p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                  <li>What causes overfitting?
                                    <ul>
                                      <li>Excessive complexity of the parameter space $\Theta$,
                                          extra degrees of freedom in the model $g(x, \theta)$ are "spent"
                                          on overly precise fit to the training sample</li>
                                      <li>Overfitting always occurs when there is optimization of parameters
                                          over a finite (inherently incomplete) sample</li>
                                    </ul>
                                  </li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="typesetting">
                        <ul>
                              <li>How to detect overfitting?
                                <ul>
                                  <li>Empirically, by splitting the sample into train and test</li>
                                </ul>
                              </li>
                              <li>It's impossible to completely get rid of it. How to minimize?
                                <ul>
                                  <li>Minimize the error on validation (see next slide: Hold Out, Leave One Out, Cross Validation),
                                      but with care!</li>
                                  <li>Put restrictions on $\theta$ (i.e. regularization)</li>
                                </ul>
                              </li>
                        </ul>
                        </div>
                    </section>
                    <section>
                        <h3>Empirical Estimates of Generalization Ability</h3>
                                <ul>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                  <li>Empirical risk on the test data (Hold Out):
                                    <ul>
                                      <li>$HO(\mu, X^\ell, X^k) = Q(\mu(X^\ell), X^k) \to \min$</li>
                                    </ul>
                                  </li>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                  <li>Leave One Out control, $L = \ell + 1$:
                                    <ul>
                                      <li>$LOO(\mu, X^\ell) = \frac1L \sum\limits_{i=1}^L \mathcal{L}(\mu(X^\ell\backslash \{x_i\}), x_i) \to \min$</li>
                                    </ul>
                                  </li>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                  <li>Cross Validation, $L = \ell + k, X^L = X^\ell_n \cup X^k_n$:
                                    <ul>
                                      <li>$CV(\mu, X^L) = \frac1{|N|} \sum\limits_{n \in N} {Q}(\mu(X^\ell_n), X^k_n) \to \min$</li>
                                    </ul>
                                  </li>
                            </div>
                        </div>
                                </ul>
                    </section>
                    <section data-background-color="#ffffff">
                        <img src="../ml_with_python/images/cross-validation.png" alt="cross-val">
                    </section>
                </section>
                <section>
                    <section>
                        <h3>Significant Events in Machine Learning</h3>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">1997: IBM Deep Blue defeats world chess champion Garry Kasparov</p>
                                <ul>
                                  <li>480 chess CPUs</li>
                                  <li>search using alpha-beta pruning modification</li>
                                  <li>two debut books</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">2004: self-driving cars competition — DARPA Grand Challenge</p>
                                <ul>
                                  <li>prize fund of $1 million</li>
                                  <li>in the first race, the winner covered only 11.8 out of 230 km</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">2006: Google Translate launched</p>
                                <ul>
                                  <li>initially <b>statistical</b> machine translation</li>
                                  <li>mobile app appeared in 2010</li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">2011: 40 years of DARPA CALO (Cognitive Assistant that Learns and Organizes) development</p>
                                <ul>
                                  <li>Apple's Siri voice assistant launched</li>
                                  <li>IBM Watson won the TV game "Jeopardy!"</li>
                                </ul>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">2011-2015: ImageNet — error rate reduced from 25% to 3.5% versus 5% in humans</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">2015: Creation of the open company OpenAI by Elon Musk and Sam Altman, pledged to invest $1 billion</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">2016: Google DeepMind beat the world champion of the game Go</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">2018: At the Christie's auction, a painting, formally drawn by AI, sold for $432,500</p>
                            </div>
                        </div>
                    </section>
                    <section>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">2020: AlphaFold 2 predicts the structure of proteins
                                    with over 90% accuracy for about two-thirds of the proteins in the dataset</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">2021: DALL-E appears — an AI system developed by OpenAI that can generate images
                                    from textual descriptions, which has potential applications in creative industries</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">2022: ChatGPT (Generative Pre-trained Transformer) developed by OpenAI —
                                the fastest-growing consumer software application in history</p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">2024: AI gets en-Nobel-ed: John Hopfield and Geoffrey Hinton receive the Nobel Prize in Physics,
                                David Baker, Demis Hassabis and John Jumper receive the Nobel Prize in Chemistry</p>
                                </p>
                            </div>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: center">
                                    Thank you for your attention!
                                </p>
                            </div>
                        </div>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>