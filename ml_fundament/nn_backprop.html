<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

		<title>Intro to neural networks and backpropagation</title>

		<link rel="stylesheet" href="../dist/reset.css">
		<link rel="stylesheet" href="../dist/reveal.css">
		<link rel="stylesheet" href="../dist/theme/sky.css">

        <!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="../plugin/highlight/monokai.css">

		<link rel="stylesheet" href="https://pyscript.net/latest/pyscript.css" />
		<script defer src="https://pyscript.net/latest/pyscript.js"></script>
		<py-config>
			terminal = false
            packages = ["numpy", "matplotlib", "scikit-learn"]
		</py-config>
        <style>
            img.round {
                border-radius: 5%;
            }
			.important {
				color: orange;
			}
        </style>
    </head>
	<body onload="totalWrapper();">
		<div class="reveal">
			<div class="slides">
                <section>
					<div>
						<img src="../ml_with_python/images/nup_logo_dark.jpeg" alt="nup_logo" />
					</div>
						<h2>Fundamentals of Machine Learning</h2>
					<div class="fragment" style="margin-bottom:20px;">
							<div class="typesetting">
								<h3>Intro to neural networks and backpropagation</h3>
								<br />
								Alex Avdiushenko <br />
								November 5, 2024
							</div>
					</div>
                </section>
                <section>
                    <section>
                        <table style="width:100%">
                          <tr>
                            <td><div align="center">
                                Images
                                <img class="round" src="../ml_with_python/images/image_net.jpg" />
                                25% $\to$ 3.5% errors versus 5% in humans
                                </div>
                            </td>
                            <td><div align="center">
                                Text
                                <img class="round" src="../ml_with_python/images/ml_def_from_chatGPT.png"/>
                                </div>
                            </td>
                            <td><div align="center">
                                Voice
                                <img class="round" src="../ml_with_python/images/voice.jpg"/>
                                </div>
                            </td>
                          </tr>
                          <tr>
                            <td><div align="center">
                                Go, 2016
                                <img class="round" src="../ml_with_python/images/alpha_go.jpg"/>
                                </div>
                            </td>
                            <td><div align="center">
                                StarCraft, 2019
                                <img class="round" src="../ml_with_python/images/starcraft.png"/>
                                </div>
                            </td>
                            <td><div align="center">
                                Protein structure, 2020
                                <img class="round" src="../ml_with_python/images/AlphaFold.png"/>
                                </div>
                            </td>
                          </tr>
                        </table>
                    </section>
                    <section>
                        <div align="center">
                            <img src="../ml_with_python/images/classification_imagenet_progress.jpeg" width="80%"
                            style="border-radius: 5%">
                            <a href="https://paperswithcode.com/sota/image-classification-on-imagenet">
                                https://paperswithcode.com/sota/image-classification-on-imagenet</a>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">
                            Linear Model (Reminder)
                        </h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">$f_j: X \to \mathbb{R}$ — numerical features</p>
                                $$a(x, w) = f(\left< w, x\right>) = f\left(\sum\limits_{j=1}^n w_j f_j(x)+b \right)$$

                                <p style="text-align: left">where $w_1, \dots, w_n \in \mathbb{R}$ — feature weights, $b$ — bias</p>

                                <p>$f(z)$ — activation function, for example, $\text{sign}(z),\ \frac{1}{1+e^{-z}},\ (z)_+$</p>

                                <div align="center">
                                    <img class="round" src="../ml_with_python/images/artificial_neuron.jpg" alt="lin_as_nn" width=600 />
                                </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">
                            Neural Implementation of Logical Functions
                        </h3>

                        <p style="text-align: left">The functions AND, OR, NOT from binary variables $x^1$ and $x^2$:</p>

                        <p>$x^1 \wedge x^2 = [x^1 + x^2 - \frac{3}{2} > 0]$<br>
                        $x^1 \vee x^2 = [x^1 + x^2 - \frac{1}{2} > 0]$<br>
                        $\neg x^1 = [-x^1 + \frac{1}{2} > 0]$
                        </p>

                        <div align="center">
                            <img class="round" src="../ml_with_python/images/and_or.jpg" alt="and_or" width=800 />
                        </div>
                    </section>
                    <section>
                        <h3>Logical XOR Function (Exclusive OR)</h3>

                        <p style="text-align: left">Function $$x^1 \bigoplus x^2 = [x^1 \neq x^2]$$
                            <b>is not implementable</b> by a single neuron. There are two ways to implement:</p>

                        <div class="fragment">
                            <div class="typesetting">
                                <ul>
                                  <li>Adding nonlinear feature:
                                     $$x^1 \bigoplus x^2 = [x^1 + x^2 - {\color{orange}2x^1 x^2} - \frac12 > 0]$$
                                  </li>
                                    <li>Network (<span style="color: orange">two-layer superposition</span>) of
                                        AND, OR, NOT functions:
                                     $$x^1 \bigoplus x^2 = [(x^1 \vee x^2) - (x^1 \wedge x^2) - \frac12 > 0]$$
                                  </li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section data-background-color="#ffffff">
                        <div align="center">
                            <img src="../ml_with_python/images/xor_nn.jpg" alt="xor_nn" width=80% />
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Expressive Power of Neural Network</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                  <li>A two-layer network in $\{0, 1\}^n$ can implement any arbitrary Boolean function</li>
                                  <li>A two-layer network in $\mathbb{R}^n$ can separate any arbitrary convex polyhedron</li>
                                  <li>A three-layer network in $\mathbb{R}^n$ can separate any polyhedral region
                                      (may not be convex or connected)</li>
                                  <li>Using linear operations and one nonlinear activation function $\sigma$,
                                      any continuous function can be approximated with any desired accuracy</li>
                                  <li>For some special classes of deep neural networks,
                                      it has been proven that they have exponentially higher expressive power
                                      than shallow networks
                                </ul>

                                <hr>
                                <a href="https://arxiv.org/pdf/1711.00811.pdf" style="text-align: left">V. Khrulkov, A. Novikov, I. Oseledets.
                                Expressive power of recurrent neural networks, Feb, ICLR 2018</a></li>
                            </div>
                        </div>
                    </section>
                    <section>
                        <p style="text-align: left">Function $\sigma(z)$ — sigmoid, if $\lim\limits_{z \to -\infty} \sigma(z) = 0$ and $\lim\limits_{z \to +\infty} \sigma(z) = 1$</p>

                        <hr>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                            <h4 style="text-align: left">Cybenko's Theorem</h4>

                            <p style="text-align: left">If $\sigma(z)$ is a continuous sigmoid,
                                then for any continuous function $f(x)$ on $[0,1]^n$,
                                there exist such parameter values $w_h \in \mathbb{R}^n, b \in \mathbb{R},
                                \alpha_h \in \mathbb{R}$ that a single-layer network
                             $a(x) = \sigma\left(\sum\limits_{h=1}^H \alpha_h \left< x, w_h\right> + b\right)$
                            approximates $f(x)$ with any desired accuracy $\varepsilon$:
                            $|a(x) - f(x)| < \varepsilon$, for all $x \in [0, 1]^n$
                            </p>
                            <hr>
                            <p>G. Cybenko. Approximation by Superpositions of a Sigmoidal Function.
                                Mathematics of Control, Signals, and Systems (MCSS) 2 (4): 303-314 (Dec 1, 1989)</p>
                            </div>
                        </div>
                    </section>
                </section>
                <section>
                    <section>
                        <p><a href="https://colab.research.google.com/github/avalur/avalur.github.io/blob/master/ml_fundament/nn_backprop_practice.ipynb">
                            Simple Neural Network Demo in PyTorch (until Micrograd)</a></p>
                    </section>
                    <section>
                        <h3>Linear Classifier</h3>

                        <p>Prediction $y_{pred} = x \cdot W + b$</p>

                        <div align="center">
                            <img src="../ml_with_python/images/linear_classification_scheme.jpg" width=800/>
                            <p>$\quad x\quad\quad\cdot\quad W\quad\quad+\quad b\quad$</p>
                        </div>
                    </section>
                    <section>
                        <h3>Ten Separating Planes</h3>

                        <div align="center">
                            <img src="../ml_with_python/images/2D_example.jpg" width=800/>
                        </div>

                        <p>In our example, the space is 784-dimensional: $\mathbb{R}^{784}$</p>
                    </section>
                    <section>
                        <div class="r-frame">
                            <p style="padding-left: 20px">
                            <b>Question:</b> How to find the best parameters:
                            the weight matrix $W$ and the bias $b$ ?
                            </p>
                        </div>
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p style="text-align: left">If $y_{true, i} \in \mathbb{R}$
                                    (that is, the task of linear regression),
                                    then to minimize the sum of the squares of differences (least squares method),
                                    the answer is calculated <b>analytically</b> by the formula:</p>
                                    $$\hat{W} = (X^TX)^{-1}X^T y_{true}$$
                            </div>
                        </div>
                    </section>
                    <section>
                        <p>In general, it is solved <b>numerically</b> by minimizing the loss function.
                            Most often by stochastic gradient descent.</p>

                        <div align="center">
                            <img class="round" src="../ml_with_python/images/momentum.jpg" width=1000/>
                        </div>

                        <a href="https://distill.pub/2017/momentum/">Distill.pub momentum</a>
                    </section>
                    <section>
                        <h3>Softmax — for Classification</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <p>We transform our responses of the linear model into class probabilities:</p>
                                $$ p(c=0| x) = \frac{e^{y_0}}{e^{y_0}+e^{y_1}+\dots+e^{y_n}} = \frac{e^{y_0}}{\sum\limits_i e^{y_i}} \\
                                   p(c=1| x) = \frac{e^{y_1}}{e^{y_0}+e^{y_1}+\dots+e^{y_n}} = \frac{e^{y_1}}{\sum\limits_i e^{y_i}} \\
                                   \dots
                                $$
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">Principle of Maximum Likelihood (Reminder)</h3>

                        $$\arg\max\limits_w {P(Y|w, X)P(w)} {\color{orange}=} \arg\max\limits_w \prod\limits_{i=1}^\ell
                        {P(y_i|w, x_i)P(w)} = \\ \arg\max\limits_w \sum\limits_{i=1}^\ell \log P(y_i|w, x_i) + \log P(w)$$

                        <h3 style="text-align: left">Minimization of Loss Function</h3>

                        $$L(w) = \sum\limits_{i=1}^\ell {\mathcal{L}(y_i, x_i, w)} = -\log P(y_i|w, x_i) \to \min\limits_w$$
                    </section>
                    <section>
                        <p style="text-align: left">
                            This is cross-entropy loss for the case $y_i \in \{0, 1\}$. In our case:</p>
                        $$ L(W, b) = - \sum\limits_j \log \frac{e^{(x_jW + b)_{y_j}}}{\sum\limits_i e^{(x_jW + b)_{i}}}$$

                        <p style="text-align: left">
                            We find the minimum of the function by stochastic gradient descent:</p>
                        $$ W^{k+1} = W^{k} - \eta \frac{\partial L}{\partial W} \\
                        b^{k+1} = b^{k} - \eta \frac{\partial L}{\partial b} \quad\quad$$
                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <div class="r-frame important">
                                    <b>Question:</b> Why is this loss function a cross-entropy loss?
                                </div>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3>Mini-batch Training</h3>

                        <p style="text-align: left;font-size: medium">We reduce the variance of the gradient.</p>

                        <p style="text-align: left"><strong>Input:</strong> sample $X^\ell$, learning rate $\eta$, forgetting rate $\lambda$</p>
                        <p style="text-align: left"><strong>Output:</strong> weight vector $w \equiv (w_{jh}, w_{hm})$</p>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ol>
                                    <li>Initialize the weights</li>
                                    <li>Initialize the functional assessment
                                    $Q = \frac{1}{\ell} \sum\limits_{i=1}^\ell \mathcal{L}_i (w)$</li>
                                </ol>

                                <p style="text-align: left"><strong>Repeat:</strong></p>
                                <ol start="3">
                                    <li>Select $M$ objects $x_i$ from $X^\ell$ at random</li>
                                    <li>Calculate the loss: $\varepsilon = \frac{1}{M} \sum\limits_{i=1}^M \mathcal{L}_i (w)$</li>
                                    <li>Take the gradient step: ${\color{orange}w = w - \eta \frac{1}{M} \sum\limits_{i=1}^M \nabla \mathcal{L}_i (w)}$</li>
                                    <li>Estimate the functional: $Q = \lambda \varepsilon + (1 - \lambda) Q$</li>
                                </ol>
                                <p style="text-align: left"><strong>Until</strong> the value of $Q$ and/or the weight $w$ converges</p>
                            </div>
                        </div>
                    </section>
                    <section data-background-color="#ffffff">
                        <h3>Neural network</h3>

                        <div align="center">
                            <img src="../ml_with_python/images/nn_scheme.jpg" width=1000/>
                        </div>
                    </section>
                    <section>
                        <h3>Summary</h3>

                        <div class="fragment" style="margin-bottom:20px;">
                            <div class="typesetting">
                                <ul>
                                    <li>Neuron = Linear classification or regression</li>
                                    <li>Neural network = Superposition of neurons with a non-linear activation function</li>
                                    <li>BackPropagation = Fast differentiation of superpositions. Allows training of networks of virtually any configuration</li>
                                    <li>Methods to improve convergence and quality:
                                      <ul>
                                      <li>Training on mini-subsets (mini-batch)</li>
                                      <li>Different activation functions</li>
                                      <li>Regularization</li>
                                      </ul>
                                    </li>
                                </ul>
                            </div>
                        </div>
                    </section>
                    <section>
                        <h3 style="text-align: left">What else can you check out?</h3>
                        <ul>
                            <li><a href="https://youtu.be/VMj-3S1tku0">The spelled-out intro to neural networks and backpropagation by Andrej Karpathy</a></li>
                            <li><a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U">Backprop on 3blue1brown</a></li>
                            <li><a href="http://cs231n.github.io/optimization-2/">Stanford course materials</a></li>
                        </ul>
                    </section>
                </section>
            </div>
        </div>
		<script src="../dist/reveal.js"></script>
		<script src="../plugin/notes/notes.js"></script>
		<script src="../plugin/markdown/markdown.js"></script>
		<script src="../plugin/highlight/highlight.js"></script>
		<script src="../plugin/math/math.js"></script>
		<script src="../scripts/utils.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				// The "normal" size of the presentation, aspect ratio will
				// be preserved when the presentation is scaled to fit different
				// resolutions. Can be specified using percentage units.
				width: '100%',
				height: '100%',
				// Factor of the display size that should remain empty around the content
				margin: 0.08,

				// Bounds for smallest/largest possible scale to apply to content
				minScale: 0.2,
				maxScale: 2.0,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath.KaTeX ]
			});

			Reveal.addEventListener('fragmentshown', function (event) {
				if (lettersAnimate) {
					[...event.fragment.getElementsByClassName('typesetting')].forEach(element => {
						playAnimation(element);
					});
				}
			});
        </script>
    </body>
</html>